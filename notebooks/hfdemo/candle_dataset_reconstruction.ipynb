{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Candle Dataset Cleaning\n",
    "\n",
    "After running the first training job on the full dataset, we can see that despite the training loss decreasing, the validation loss is remaining fairly constant. As a potential remedy, we are going to try removing all of the rows from a closed market from the dataset. This will have a few advantages:\n",
    "1. It will drasitcally reduce the size of the data by approximately 55 - 60%, by keeping only the examples that are relevant to the task we need to perform. \n",
    "2. It will ensure that the loss metrics are mearusing the models performnace on market data where trading is occuring. Currently, the loss function is likely dilluted, as so much of the data is just a flat time-series, that the model is likely performing perfectly on these examples, causing the loss metric to be diluted, and likely not allowing a high enough gradient to be built up for the backward pass.\n",
    "\n",
    "## Plan for Cleaning Data \n",
    "\n",
    "1. Remove any rows where the market is closed. We will never be using this in the actual model.\n",
    "2. This should give us a a couple hundred time-series per ticker. We want to create a time-series ID for each continuous time series in the dataset. This will be used as a column ID.\n",
    "3. Then iterate over each ticker. For each ticker:\n",
    " - Use our date indicices, to create a train, validation, and test set for that ticker. Be sure that the date the sets do not slice any time-series (e.g. each of the 3 sets should have a unique set of time-series IDs)\n",
    " - Append eacah of the three sets to a master train, validation, and test set respectively. \n",
    " - These steps will replace the current `select_by_index` usage\n",
    "4. Train the `TimeSeriesPreprocessor` on the train set\n",
    "5. Create the train, validation, and test datasets, using the trained preprocessor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Third Party\n",
    "from transformers import (\n",
    "    EarlyStoppingCallback,\n",
    "    PatchTSMixerConfig,\n",
    "    PatchTSMixerForPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First Party\n",
    "from tsfm_public.toolkit.dataset import ForecastDFDataset\n",
    "from tsfm_public.toolkit.time_series_preprocessor import TimeSeriesPreprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Dataset from the CSV file\n",
    "DATA_DIR = \"/home/ubuntu/verb-workspace/data\" # set this accordingly to the location of the data\n",
    "DATASET = \"10s-candles-2023.csv\"\n",
    "\n",
    "DATASET_PATH = os.path.join(DATA_DIR, DATASET)\n",
    "timestamp_col = 't'\n",
    "\n",
    "data = pd.read_csv(\n",
    "    DATASET_PATH,\n",
    "    parse_dates=[timestamp_col]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31160015 entries, 0 to 31160014\n",
      "Data columns (total 10 columns):\n",
      " #   Column                             Dtype              \n",
      "---  ------                             -----              \n",
      " 0   t                                  datetime64[ns, UTC]\n",
      " 1   targ_o                             float32            \n",
      " 2   targ_h                             float32            \n",
      " 3   targ_l                             float32            \n",
      " 4   targ_c                             float32            \n",
      " 5   targ_v                             float32            \n",
      " 6   ticker                             category           \n",
      " 7   market_state_MarketState.CLOSED    int8               \n",
      " 8   market_state_MarketState.EXTENDED  int8               \n",
      " 9   market_state_MarketState.OPEN      int8               \n",
      "dtypes: category(1), datetime64[ns, UTC](1), float32(5), int8(3)\n",
      "memory usage: 950.9 MB\n"
     ]
    }
   ],
   "source": [
    "# To start, let's reset the types on the columns to ensure that the dataset size is compressed\n",
    "\n",
    "# Convert float64 columns to float32\n",
    "float_columns = data.select_dtypes(include=['float64']).columns\n",
    "data[float_columns] = data[float_columns].astype('float32')\n",
    "\n",
    "# Convert object columns to category\n",
    "object_columns = data.select_dtypes(include=['object']).columns\n",
    "data[object_columns] = data[object_columns].astype('category')\n",
    "\n",
    "int_columns = data.select_dtypes(include=['int64']).columns\n",
    "data[int_columns] = data[int_columns].astype('int8')\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14488415 entries, 0 to 14488414\n",
      "Data columns (total 11 columns):\n",
      " #   Column                             Dtype              \n",
      "---  ------                             -----              \n",
      " 0   t                                  datetime64[ns, UTC]\n",
      " 1   targ_o                             float32            \n",
      " 2   targ_h                             float32            \n",
      " 3   targ_l                             float32            \n",
      " 4   targ_c                             float32            \n",
      " 5   targ_v                             float32            \n",
      " 6   ticker                             category           \n",
      " 7   market_state_MarketState.CLOSED    int8               \n",
      " 8   market_state_MarketState.EXTENDED  int8               \n",
      " 9   market_state_MarketState.OPEN      int8               \n",
      " 10  date_string                        object             \n",
      "dtypes: category(1), datetime64[ns, UTC](1), float32(5), int8(3), object(1)\n",
      "memory usage: 552.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# Now, let's trim down the dataset, by removing all of the Market Closed columns, this will create several individual time series for each ticker. \n",
    "# We can then add a date_string column, to use a second ID column for each time-series\n",
    "\n",
    "# Remove all rows where the market is closed and reset the index\n",
    "data = data[data['market_state_MarketState.CLOSED'] != 1].reset_index(drop=True)\n",
    "\n",
    "# Add a date_string column to use as a second ID for each time series\n",
    "data['date_string'] = data[timestamp_col].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, even after adding the additional column, we have reduced the size of the dataset again by almost 50%. We can now remove all of the `market_state` columns, to ensure that we are ready for the full processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14488415 entries, 0 to 14488414\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Dtype              \n",
      "---  ------       -----              \n",
      " 0   t            datetime64[ns, UTC]\n",
      " 1   targ_o       float32            \n",
      " 2   targ_h       float32            \n",
      " 3   targ_l       float32            \n",
      " 4   targ_c       float32            \n",
      " 5   targ_v       float32            \n",
      " 6   ticker       category           \n",
      " 7   date_string  object             \n",
      "dtypes: category(1), datetime64[ns, UTC](1), float32(5), object(1)\n",
      "memory usage: 511.2+ MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Remove the unused cols\n",
    "try:\n",
    "    data.drop(columns=[\"market_state_MarketState.CLOSED\", \"market_state_MarketState.OPEN\", \"market_state_MarketState.EXTENDED\"], inplace=True)\n",
    "except KeyError:\n",
    "    pass\n",
    "\n",
    "# Forward fill any straggler NA values\n",
    "data.ffill(inplace=True)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique ticker-date pairs in train set: 2689\n"
     ]
    }
   ],
   "source": [
    "data_ticker_dates = data[['ticker', 'date_string']].drop_duplicates()\n",
    "\n",
    "print(f\"Number of unique ticker-date pairs in train set: {len(data_ticker_dates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (13027535, 8)\n",
      "Validation data shape: (739440, 8)\n",
      "Test data shape: (721440, 8)\n"
     ]
    }
   ],
   "source": [
    "# We can't use the train_test_split method, to create our different sets, as it simply create a train test split for each of the 2689 unique pairs.\n",
    "# The best way to create a train test split, is to take 90% of the groups for training, 5% for validation, and 5% for testing. \n",
    "\n",
    "\n",
    "# Split the ticker-date pairs into train, valid, and test sets\n",
    "train_ticker_dates, temp_ticker_dates = train_test_split(data_ticker_dates, test_size=0.1, random_state=SEED)\n",
    "valid_ticker_dates, test_ticker_dates = train_test_split(temp_ticker_dates, test_size=0.5, random_state=SEED)\n",
    "\n",
    "# Create train, valid, and test dataframes based on the ticker-date pairs\n",
    "train_data = data[data.set_index(['ticker', 'date_string']).index.isin(train_ticker_dates.set_index(['ticker', 'date_string']).index)]\n",
    "valid_data = data[data.set_index(['ticker', 'date_string']).index.isin(valid_ticker_dates.set_index(['ticker', 'date_string']).index)]\n",
    "test_data = data[data.set_index(['ticker', 'date_string']).index.isin(test_ticker_dates.set_index(['ticker', 'date_string']).index)]\n",
    "\n",
    "print(f\"Train data shape: {train_data.shape}\")\n",
    "print(f\"Validation data shape: {valid_data.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique ticker-date pairs in train set: 2420\n",
      "Number of unique ticker-date pairs in valid set: 134\n",
      "Number of unique ticker-date pairs in test set: 135\n"
     ]
    }
   ],
   "source": [
    "# Validation of the ticker date-strings\n",
    "\n",
    "# Get all unique pairs of ticker and date_string for the train data frame\n",
    "validated_train_ticker_dates = train_data[['ticker', 'date_string']].drop_duplicates()\n",
    "validated_valid_ticker_dates = valid_data[['ticker', 'date_string']].drop_duplicates()\n",
    "validated_test_ticker_dates = test_data[['ticker', 'date_string']].drop_duplicates()\n",
    "\n",
    "print(f\"Number of unique ticker-date pairs in train set: {len(validated_train_ticker_dates)}\")\n",
    "print(f\"Number of unique ticker-date pairs in valid set: {len(validated_valid_ticker_dates)}\")\n",
    "print(f\"Number of unique ticker-date pairs in test set: {len(validated_test_ticker_dates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap between train and validation sets: 0\n",
      "Overlap between train and test sets: 0\n",
      "Overlap between validation and test sets: 0\n"
     ]
    }
   ],
   "source": [
    "# Verify no overlap between train and validation sets\n",
    "train_valid_overlap = validated_train_ticker_dates.merge(validated_valid_ticker_dates, on=['ticker', 'date_string'])\n",
    "print(f\"Overlap between train and validation sets: {len(train_valid_overlap)}\")\n",
    "\n",
    "# Verify no overlap between train and test sets\n",
    "train_test_overlap = validated_train_ticker_dates.merge(validated_test_ticker_dates, on=['ticker', 'date_string'])\n",
    "print(f\"Overlap between train and test sets: {len(train_test_overlap)}\")\n",
    "\n",
    "# Verify no overlap between validation and test sets\n",
    "valid_test_overlap = validated_valid_ticker_dates.merge(validated_test_ticker_dates, on=['ticker', 'date_string'])\n",
    "print(f\"Overlap between validation and test sets: {len(valid_test_overlap)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 13027535 entries, 0 to 14488414\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Dtype              \n",
      "---  ------       -----              \n",
      " 0   t            datetime64[ns, UTC]\n",
      " 1   targ_o       float32            \n",
      " 2   targ_h       float32            \n",
      " 3   targ_l       float32            \n",
      " 4   targ_c       float32            \n",
      " 5   targ_v       float32            \n",
      " 6   ticker       category           \n",
      " 7   date_string  object             \n",
      "dtypes: category(1), datetime64[ns, UTC](1), float32(5), object(1)\n",
      "memory usage: 559.1+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 739440 entries, 137880 to 14313218\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count   Dtype              \n",
      "---  ------       --------------   -----              \n",
      " 0   t            739440 non-null  datetime64[ns, UTC]\n",
      " 1   targ_o       739440 non-null  float32            \n",
      " 2   targ_h       739440 non-null  float32            \n",
      " 3   targ_l       739440 non-null  float32            \n",
      " 4   targ_c       739440 non-null  float32            \n",
      " 5   targ_v       739440 non-null  float32            \n",
      " 6   ticker       739440 non-null  category           \n",
      " 7   date_string  739440 non-null  object             \n",
      "dtypes: category(1), datetime64[ns, UTC](1), float32(5), object(1)\n",
      "memory usage: 31.7+ MB\n"
     ]
    }
   ],
   "source": [
    "valid_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 721440 entries, 207000 to 14347778\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count   Dtype              \n",
      "---  ------       --------------   -----              \n",
      " 0   t            721440 non-null  datetime64[ns, UTC]\n",
      " 1   targ_o       721440 non-null  float32            \n",
      " 2   targ_h       721440 non-null  float32            \n",
      " 3   targ_l       721440 non-null  float32            \n",
      " 4   targ_c       721440 non-null  float32            \n",
      " 5   targ_v       721440 non-null  float32            \n",
      " 6   ticker       721440 non-null  category           \n",
      " 7   date_string  721440 non-null  object             \n",
      "dtypes: category(1), datetime64[ns, UTC](1), float32(5), object(1)\n",
      "memory usage: 31.0+ MB\n"
     ]
    }
   ],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = os.path.join(DATA_DIR, \"10s-candles-train.csv\")\n",
    "valid_dataset_path = os.path.join(DATA_DIR, \"10s-candles-valid.csv\")\n",
    "test_dataset_path = os.path.join(DATA_DIR, \"10s-candles-test.csv\")\n",
    "\n",
    "train_data.to_csv(train_dataset_path, index=False)\n",
    "valid_data.to_csv(valid_dataset_path, index=False)\n",
    "test_data.to_csv(test_dataset_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-kernel",
   "language": "python",
   "name": "venv-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
