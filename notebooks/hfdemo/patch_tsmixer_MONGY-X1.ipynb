{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7478e0e2-b7af-4fd4-b44e-ca58e0c31b71",
   "metadata": {},
   "source": [
    "# MONGY: Training `PatchTSMixer` on Financial Candlestick Data\n",
    "## Direct forecasting example\n",
    "\n",
    "This notebooke demonstrates the usage of a `PatchTSMixer` model for a multivariate time series forecasting task. This notebook has a dependecy on HuggingFace [transformers](https://github.com/huggingface/transformers) repo. For details related to model architecture, refer to the [TSMixer paper](https://arxiv.org/abs/2306.09364)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f63ae353-96df-4380-89f6-1e6cebf684fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Third Party\n",
    "from transformers import (\n",
    "    EarlyStoppingCallback,\n",
    "    PatchTSMixerConfig,\n",
    "    PatchTSMixerForPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# First Party\n",
    "from tsfm_public.toolkit.dataset import ForecastDFDataset\n",
    "from tsfm_public.toolkit.time_series_preprocessor import TimeSeriesPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a826c4f3-1c6c-4088-b6af-f430f45fd380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27bb0d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.utils.data.dataloader\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4eb9be-c19f-448f-a4bd-c600e068633f",
   "metadata": {},
   "source": [
    "## Load and prepare datasets\n",
    "\n",
    "In the next cell, please adjust the following parameters to suit your application:\n",
    "- `dataset_path`: path to local .csv file, or web address to a csv file for the data of interest. Data is loaded with pandas, so anything supported by\n",
    "`pd.read_csv` is supported: (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html).\n",
    "- `timestamp_column`: column name containing timestamp information, use None if there is no such column\n",
    "- `id_columns`: List of column names specifying the IDs of different time series. If no ID column exists, use []\n",
    "- `forecast_columns`: List of columns to be modeled\n",
    "- `context_length`: The amount of historical data used as input to the model. Windows of the input time series data with length equal to\n",
    "context_length will be extracted from the input dataframe. In the case of a multi-time series dataset, the context windows will be created\n",
    "so that they are contained within a single time series (i.e., a single ID).\n",
    "- `forecast_horizon`: Number of time stamps to forecast in future.\n",
    "- `train_start_index`, `train_end_index`: the start and end indices in the loaded data which delineate the training data.\n",
    "- `valid_start_index`, `valid_end_index`: the start and end indices in the loaded data which delineate the validation data.\n",
    "- `test_start_index`, `test_end_index`: the start and end indices in the loaded data which delineate the test data.\n",
    "- `patch_length`: The patch length for the `PatchTSMixer` model. Recommended to have a value so that `context_length` is divisible by it.\n",
    "- `num_workers`: Number of dataloder workers in pytorch dataloader.\n",
    "- `batch_size`: Batch size. \n",
    "The data is first loaded into a Pandas dataframe and split into training, validation, and test parts. Then the pandas dataframes are converted\n",
    "to the appropriate torch dataset needed for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4c1e812-f2d6-4ccb-a79c-47879b562d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to setup our context, horizon, and patch size based on our task. We want to use\n",
    "# 4 hours of lookback to start, in order to predict the next 5 minutes of candles. Regarding\n",
    "# patch length, we know that we will want a larger patch size, so we will start with 64 as\n",
    "# a base case assumption\n",
    "context_length = 60 * 4  # This will give us 4 hours of lookback (4 hours * 60 min per hour)\n",
    "forecast_horizon = 3 # This will give us 3 minutes of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ba7a031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Dataset from the CSV file\n",
    "DATA_DIR = \"/home/ubuntu/verb-workspace/data\"\n",
    "\n",
    "TRAIN_DATASET = f\"{DATA_DIR}/1min-candles-train-w-CANDLES.csv\"\n",
    "VALID_DATASET = f\"{DATA_DIR}/1min-candles-valid-w-CANDLES.csv\"\n",
    "TEST_DATASET = f\"{DATA_DIR}/1min-candles-test-w-CANDLES.csv\"\n",
    "\n",
    "timestamp_col = 't'\n",
    "\n",
    "train_data = pd.read_csv(\n",
    "    TRAIN_DATASET,\n",
    "    parse_dates=[timestamp_col]\n",
    ")\n",
    "\n",
    "valid_data = pd.read_csv(\n",
    "    VALID_DATASET,\n",
    "    parse_dates=[timestamp_col]\n",
    ")\n",
    "\n",
    "test_data = pd.read_csv(\n",
    "    TEST_DATASET,\n",
    "    parse_dates=[timestamp_col]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a125a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values\n",
    "assert sum(train_data.isna().sum().to_list()) == 0\n",
    "assert sum(valid_data.isna().sum().to_list()) == 0\n",
    "assert sum(test_data.isna().sum().to_list()) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94bd97e3-4fbc-44b0-b464-a96eaa50c3f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date_string</th>\n",
       "      <th>t</th>\n",
       "      <th>targ_o</th>\n",
       "      <th>targ_h</th>\n",
       "      <th>targ_l</th>\n",
       "      <th>targ_c</th>\n",
       "      <th>targ_v</th>\n",
       "      <th>targ_vwap</th>\n",
       "      <th>targ_red</th>\n",
       "      <th>targ_green</th>\n",
       "      <th>cont_market_open</th>\n",
       "      <th>cont_market_extended</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>2023-01-03 05:30:00-05:00</td>\n",
       "      <td>130.80</td>\n",
       "      <td>130.8000</td>\n",
       "      <td>130.800</td>\n",
       "      <td>130.800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>2023-01-03 05:31:00-05:00</td>\n",
       "      <td>130.80</td>\n",
       "      <td>130.8000</td>\n",
       "      <td>130.800</td>\n",
       "      <td>130.800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>2023-01-03 05:32:00-05:00</td>\n",
       "      <td>130.80</td>\n",
       "      <td>130.8000</td>\n",
       "      <td>130.800</td>\n",
       "      <td>130.800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>2023-01-03 05:33:00-05:00</td>\n",
       "      <td>130.80</td>\n",
       "      <td>130.8000</td>\n",
       "      <td>130.800</td>\n",
       "      <td>130.800</td>\n",
       "      <td>235.0</td>\n",
       "      <td>130.8009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>2023-01-03 05:34:00-05:00</td>\n",
       "      <td>130.80</td>\n",
       "      <td>130.8000</td>\n",
       "      <td>130.800</td>\n",
       "      <td>130.800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937435</th>\n",
       "      <td>V</td>\n",
       "      <td>2023-11-17</td>\n",
       "      <td>2023-11-17 15:55:00-05:00</td>\n",
       "      <td>249.65</td>\n",
       "      <td>249.7277</td>\n",
       "      <td>249.620</td>\n",
       "      <td>249.680</td>\n",
       "      <td>20222.0</td>\n",
       "      <td>249.6721</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937436</th>\n",
       "      <td>V</td>\n",
       "      <td>2023-11-17</td>\n",
       "      <td>2023-11-17 15:56:00-05:00</td>\n",
       "      <td>249.67</td>\n",
       "      <td>249.7700</td>\n",
       "      <td>249.670</td>\n",
       "      <td>249.705</td>\n",
       "      <td>24402.0</td>\n",
       "      <td>249.7157</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937437</th>\n",
       "      <td>V</td>\n",
       "      <td>2023-11-17</td>\n",
       "      <td>2023-11-17 15:57:00-05:00</td>\n",
       "      <td>249.71</td>\n",
       "      <td>249.7600</td>\n",
       "      <td>249.670</td>\n",
       "      <td>249.725</td>\n",
       "      <td>29366.0</td>\n",
       "      <td>249.7169</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937438</th>\n",
       "      <td>V</td>\n",
       "      <td>2023-11-17</td>\n",
       "      <td>2023-11-17 15:58:00-05:00</td>\n",
       "      <td>249.73</td>\n",
       "      <td>249.7300</td>\n",
       "      <td>249.655</td>\n",
       "      <td>249.660</td>\n",
       "      <td>29316.0</td>\n",
       "      <td>249.7011</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937439</th>\n",
       "      <td>V</td>\n",
       "      <td>2023-11-17</td>\n",
       "      <td>2023-11-17 15:59:00-05:00</td>\n",
       "      <td>249.66</td>\n",
       "      <td>249.6600</td>\n",
       "      <td>249.520</td>\n",
       "      <td>249.590</td>\n",
       "      <td>83249.0</td>\n",
       "      <td>249.5993</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>937440 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ticker date_string                          t  targ_o    targ_h  \\\n",
       "0        AAPL  2023-01-03  2023-01-03 05:30:00-05:00  130.80  130.8000   \n",
       "1        AAPL  2023-01-03  2023-01-03 05:31:00-05:00  130.80  130.8000   \n",
       "2        AAPL  2023-01-03  2023-01-03 05:32:00-05:00  130.80  130.8000   \n",
       "3        AAPL  2023-01-03  2023-01-03 05:33:00-05:00  130.80  130.8000   \n",
       "4        AAPL  2023-01-03  2023-01-03 05:34:00-05:00  130.80  130.8000   \n",
       "...       ...         ...                        ...     ...       ...   \n",
       "937435      V  2023-11-17  2023-11-17 15:55:00-05:00  249.65  249.7277   \n",
       "937436      V  2023-11-17  2023-11-17 15:56:00-05:00  249.67  249.7700   \n",
       "937437      V  2023-11-17  2023-11-17 15:57:00-05:00  249.71  249.7600   \n",
       "937438      V  2023-11-17  2023-11-17 15:58:00-05:00  249.73  249.7300   \n",
       "937439      V  2023-11-17  2023-11-17 15:59:00-05:00  249.66  249.6600   \n",
       "\n",
       "         targ_l   targ_c   targ_v  targ_vwap  targ_red  targ_green  \\\n",
       "0       130.800  130.800      0.0     0.0000         0           0   \n",
       "1       130.800  130.800      0.0     0.0000         0           0   \n",
       "2       130.800  130.800      0.0     0.0000         0           0   \n",
       "3       130.800  130.800    235.0   130.8009         0           0   \n",
       "4       130.800  130.800      0.0     0.0000         0           0   \n",
       "...         ...      ...      ...        ...       ...         ...   \n",
       "937435  249.620  249.680  20222.0   249.6721         0           1   \n",
       "937436  249.670  249.705  24402.0   249.7157         0           1   \n",
       "937437  249.670  249.725  29366.0   249.7169         0           1   \n",
       "937438  249.655  249.660  29316.0   249.7011         1           0   \n",
       "937439  249.520  249.590  83249.0   249.5993         1           0   \n",
       "\n",
       "        cont_market_open  cont_market_extended  \n",
       "0                      0                     1  \n",
       "1                      0                     1  \n",
       "2                      0                     1  \n",
       "3                      0                     1  \n",
       "4                      0                     1  \n",
       "...                  ...                   ...  \n",
       "937435                 1                     0  \n",
       "937436                 1                     0  \n",
       "937437                 1                     0  \n",
       "937438                 1                     0  \n",
       "937439                 1                     0  \n",
       "\n",
       "[937440 rows x 13 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f28412d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Train\n",
      "Done Valid\n",
      "Done Test\n"
     ]
    }
   ],
   "source": [
    "\n",
    "id_columns = ['ticker', 'date_string']\n",
    "forecast_columns = ['targ_o', 'targ_c', 'targ_h', 'targ_l', 'targ_v', 'targ_vwap', 'targ_red', 'targ_green']\n",
    "control_columns = ['cont_market_open', 'cont_market_extended']\n",
    "\n",
    "train_tsp = TimeSeriesPreprocessor(\n",
    "    timestamp_column=timestamp_col,\n",
    "    id_columns=id_columns,\n",
    "    target_columns=forecast_columns,\n",
    "    control_columns=control_columns,\n",
    "    scaling=True,\n",
    ")\n",
    "train_tsp.train(train_data)\n",
    "print(\"Done Train\")\n",
    "\n",
    "valid_tsp = TimeSeriesPreprocessor(\n",
    "    timestamp_column=timestamp_col,\n",
    "    id_columns=id_columns,\n",
    "    target_columns=forecast_columns,\n",
    "    control_columns=control_columns,\n",
    "    scaling=True,\n",
    ")\n",
    "valid_tsp.train(valid_data)\n",
    "print(\"Done Valid\")\n",
    "\n",
    "test_tsp = TimeSeriesPreprocessor(\n",
    "    timestamp_column=timestamp_col,\n",
    "    id_columns=id_columns,\n",
    "    target_columns=forecast_columns,\n",
    "    control_columns=control_columns,\n",
    "    scaling=True,\n",
    ")\n",
    "test_tsp.train(test_data)\n",
    "print(\"Done Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "678d849d-41fc-450d-a855-1dde27179b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ForecastDFDataset(\n",
    "    train_tsp.preprocess(train_data),\n",
    "    id_columns=id_columns,\n",
    "    target_columns=forecast_columns,\n",
    "    control_columns=control_columns,\n",
    "    context_length=context_length,\n",
    "    prediction_length=forecast_horizon,\n",
    ")\n",
    "valid_dataset = ForecastDFDataset(\n",
    "    valid_tsp.preprocess(valid_data),\n",
    "    id_columns=id_columns,\n",
    "    target_columns=forecast_columns,\n",
    "    control_columns=control_columns,\n",
    "    context_length=context_length,\n",
    "    prediction_length=forecast_horizon,\n",
    ")\n",
    "test_dataset = ForecastDFDataset(\n",
    "    test_tsp.preprocess(test_data),\n",
    "    id_columns=id_columns,\n",
    "    target_columns=forecast_columns,\n",
    "    control_columns=control_columns,\n",
    "    context_length=context_length,\n",
    "    prediction_length=forecast_horizon,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d126150d-cb95-4d59-a401-2499c0a402e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "# Indices for accessing the OHLC values in the tensors\n",
    "I_OPEN = 0\n",
    "I_CLOSE = 1\n",
    "I_HIGH = 2\n",
    "I_LOW = 3\n",
    "I_VOLUME = 4\n",
    "I_VWAP = 5\n",
    "I_RED = 6\n",
    "I_GREEN = 7\n",
    "\n",
    "\n",
    "def _weighted_average(tensor):\n",
    "    # Get the shape of the tensor\n",
    "    shape = tensor.shape\n",
    "    \n",
    "    # Create a tensor of weights based on the last dimension\n",
    "    weights = torch.arange(shape[-1], 0, -1, dtype=tensor.dtype, device=tensor.device)\n",
    "    \n",
    "    # Multiply the tensor by the weights along the last dimension\n",
    "    weighted_tensor = tensor * weights\n",
    "    \n",
    "    # Sum the weighted tensor along the last dimension\n",
    "    sum_weighted_tensor = torch.sum(weighted_tensor, dim=-1)\n",
    "    \n",
    "    # Sum the weights\n",
    "    sum_weights = torch.sum(weights)\n",
    "    \n",
    "    # Compute the weighted average\n",
    "    weighted_avg = sum_weighted_tensor / sum_weights\n",
    "    \n",
    "    return weighted_avg\n",
    "    \n",
    "\n",
    "def theta_body(y_pred: torch.Tensor, y_obs: torch.Tensor) -> torch.Tensor:\n",
    "    # Create the series of closes\n",
    "    real_candle_closes = y_obs[..., I_CLOSE]\n",
    "    forecasted_candle_closes = y_pred[..., I_CLOSE]\n",
    "\n",
    "    # Get the series of opens\n",
    "    real_candle_opens = y_obs[..., I_OPEN]\n",
    "    forecasted_candle_opens = y_pred[..., I_OPEN]\n",
    "\n",
    "    # Get the series of candle bodies\n",
    "    real_bodies = real_candle_closes - real_candle_opens\n",
    "    forecasted_bodies = forecasted_candle_closes - forecasted_candle_opens\n",
    "\n",
    "    # Get the error of each body\n",
    "    error = real_bodies - forecasted_bodies\n",
    "\n",
    "    sq_error = _weighted_average(torch.square(error))\n",
    "    abs_error = _weighted_average(torch.abs(error))\n",
    "    return sq_error, abs_error\n",
    "\n",
    "def theta_single_pnl(x: torch.Tensor, y_pred: torch.Tensor, y_obs: torch.Tensor) -> torch.Tensor:\n",
    "    # Get the close of the last real candle\n",
    "    last_candle_close = x[..., -1, I_CLOSE]\n",
    "    real_last_candle_close = y_obs[..., -1, I_CLOSE]\n",
    "    real_forecasted_candle_close = y_pred[..., -1, I_CLOSE]\n",
    "\n",
    "    # Compute if the position should be long or short, based on the real values\n",
    "    is_long = real_last_candle_close > last_candle_close\n",
    "\n",
    "    # Compute P/L of long position\n",
    "    pnl_long_real = real_last_candle_close - last_candle_close\n",
    "    pnl_long_forecasted = real_forecasted_candle_close - last_candle_close\n",
    "    # Compute P/L of short position\n",
    "    pnl_short_real = last_candle_close - real_last_candle_close\n",
    "    pnl_short_forecasted = last_candle_close - real_forecasted_candle_close\n",
    "\n",
    "    # Compute the P/L of the real candles vs forecasted candles\n",
    "    pnl_real = torch.where(is_long, pnl_long_real, pnl_long_forecasted)\n",
    "    pnl_forecasted = torch.where(is_long, pnl_short_real, pnl_short_forecasted)\n",
    "\n",
    "    error = pnl_real - pnl_forecasted\n",
    "\n",
    "    sq_error = torch.square(error)\n",
    "    abs_error = torch.abs(error)\n",
    "    return sq_error, abs_error\n",
    "\n",
    "def base_error(y_pred: torch.Tensor, y_obs: torch.Tensor) -> Tuple[torch.Tensor]:\n",
    "    # Place a mask over y_obs, to ensure it is the same size as y_pred\n",
    "    y_obs = y_obs[..., :y_pred.shape[-1]]\n",
    "    \n",
    "    error = y_obs - y_pred\n",
    "\n",
    "    raw_sq_error = torch.mean(torch.square(error), dim=-1)\n",
    "    raw_ae_error = torch.mean((torch.abs(error)), dim=-1)\n",
    "\n",
    "    sq_error = _weighted_average(raw_sq_error)\n",
    "    ae_error = _weighted_average(raw_ae_error)\n",
    "    \n",
    "    return sq_error, ae_error\n",
    "\n",
    "\n",
    "def custom_loss(x: torch.Tensor, y_pred: torch.Tensor, y_obs: torch.Tensor) -> torch.Tensor:\n",
    "    # Compute PNL rediual for each candle\n",
    "    mse, mae = base_error(y_pred, y_obs)\n",
    "    pnl_se, pnl_ae = theta_single_pnl(x, y_pred, y_obs)\n",
    "    body_se, body_ae = theta_body(y_pred, y_obs)\n",
    "\n",
    "    custom_mse = torch.mean(mse + pnl_se + body_se)\n",
    "    custom_mae = torch.mean(mae + pnl_ae + body_ae)\n",
    "\n",
    "    return (custom_mse + custom_mae) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba454c4c-db32-4c4f-8fda-3713c1be1cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "class MongyModel(PatchTSMixerForPrediction):\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        past_values: torch.Tensor,\n",
    "        future_values: Optional[torch.Tensor] = None,\n",
    "        observed_mask: Optional[torch.Tensor] = None,\n",
    "        output_hidden_states: Optional[bool] = False,\n",
    "        return_loss: bool = True,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "        # Call the parent class's forward method to get the model's outputs\n",
    "        outputs = super().forward(\n",
    "            past_values,\n",
    "            observed_mask=observed_mask,\n",
    "            future_values=future_values,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_loss=False,  # Set return_loss to False to prevent the built-in loss computation\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        # # Snap the candles to the correct opening positions, before computing the loss\n",
    "        # # This is the \"training wheels\" for the head. By helping the model with the portion\n",
    "        # # of it's task that we can help with, we severly limit the task that is posed to the\n",
    "        # # model\n",
    "        _outputs = outputs.prediction_outputs\n",
    "        \n",
    "        last_context_close = past_values[..., -1, I_CLOSE]\n",
    "        first_candle_open = _outputs[..., 0, I_OPEN]\n",
    "        first_candle_delta = last_context_close - first_candle_open\n",
    "        first_candle_delta = first_candle_delta.unsqueeze(-1).unsqueeze(-1)\n",
    "        _outputs[..., 0:4] = _outputs[..., 0:4] + first_candle_delta\n",
    "\n",
    "\n",
    "        first_candle_close = _outputs[..., 0, I_CLOSE]\n",
    "        second_candle_open = _outputs[..., 1, I_OPEN]\n",
    "        second_candle_delta = first_candle_close - second_candle_open\n",
    "        second_candle_delta = second_candle_delta.unsqueeze(-1).unsqueeze(-1)\n",
    "        _outputs[..., -2:, 0:4] = _outputs[..., -2:, 0:4] + second_candle_delta\n",
    "\n",
    "        second_candle_close = _outputs[..., 1, I_CLOSE]\n",
    "        third_candle_open = _outputs[..., 2, I_OPEN]\n",
    "        third_candle_delta = second_candle_close - third_candle_open\n",
    "        third_candle_delta = third_candle_delta.unsqueeze(-1)\n",
    "        _outputs[..., -1, 0:4] = _outputs[..., -1, 0:4] + third_candle_delta\n",
    "        \n",
    "        # Apply your custom loss function\n",
    "        loss_val = None\n",
    "        if future_values is not None and return_loss:\n",
    "            loss_val = custom_loss(\n",
    "                past_values[..., self.prediction_channel_indices],    \n",
    "                outputs.prediction_outputs[..., self.prediction_channel_indices],\n",
    "                future_values[..., self.prediction_channel_indices]\n",
    "            )\n",
    "            outputs.loss = loss_val\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (outputs.prediction_outputs,) + outputs[2:]\n",
    "            return ((loss_val,) + output) if loss_val is not None else output\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19456329-1293-45bf-99c7-e5ccf0534846",
   "metadata": {},
   "source": [
    "## Training `PatchTSMixer` From Scratch\n",
    "\n",
    "Adjust the following model parameters according to need.\n",
    "- `d_model` (`int`, *optional*, defaults to 8):\n",
    "    Hidden dimension of the model. Recommended to set it as a multiple of patch_length (i.e. 2-8X of\n",
    "    patch_len). Larger value indicates more complex model.\n",
    "- `expansion_factor` (`int`, *optional*, defaults to 2):\n",
    "    Expansion factor to use inside MLP. Recommended range is 2-5. Larger value indicates more complex model.\n",
    "- `num_layers` (`int`, *optional*, defaults to 3):\n",
    "    Number of layers to use. Recommended range is 3-15. Larger value indicates more complex model.\n",
    "- `mode`: (`str`, either to 'common_channel' or `mix_channel`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "226b904e-1ab2-478b-98b4-ce99bc23f1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_length = 16\n",
    "stride_length = 1\n",
    "\n",
    "prediction_channel_indicies = train_tsp.prediction_channel_indices\n",
    "num_input_channels = train_tsp.num_input_channels\n",
    "\n",
    "config = PatchTSMixerConfig(\n",
    "    # Dataset Kwargs\n",
    "    context_length=context_length,\n",
    "    prediction_length=forecast_horizon,\n",
    "    prediction_channel_indices=prediction_channel_indicies,\n",
    "    patch_length=patch_length,\n",
    "    num_input_channels=num_input_channels,\n",
    "    patch_stride=stride_length,\n",
    "\n",
    "    # Model Kwargs\n",
    "    d_model=6 * patch_length,\n",
    "    num_layers=3,\n",
    "    expansion_factor=4,\n",
    "    dropout=0.5,\n",
    "    head_dropout=0.7,\n",
    "    mode=\"mix_channel\",\n",
    "    scaling=None,\n",
    ")\n",
    "model = MongyModel(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fae7e0c",
   "metadata": {},
   "source": [
    "# Training Run Summaries\n",
    "\n",
    "**Run 1**: (N/A)\n",
    "This run used the full year of data, and was used as a baseline to establish that the `mix_channel` mode is more effective for our task. Additionally, all subsequent runs have been updated, to instead use only the first three months of data as training data. Thus, while the loss for this run is lower, it is not indicaitve of the paramters being a better fit, just a result of having a larger dataset.\n",
    "\n",
    "**Run 2** (0.108476):\n",
    "This run was the first in which only the first two months of data was used as a training set. March was then split in half to form the validation and test sets. Additionally, the context window was expanded, to include the last four hours of data. While this wasn't explicitly compared against a shorter context window with the same dataset, the results of the paper provide an incredibly strong suggestions towards this approach yielding more effective performance.\n",
    "\n",
    "**Run 3** (0.108230):\n",
    "This run included involved increasing the `num_layers` argument from 3 to 5. This adds additional layers to the model, giving it more of an ability to percieve complex patterns in the financial data. This results in a larger model, but hopefully, will allow the model to better understand the nuances of the highly complex financial data it is being trained on.\n",
    "\n",
    "**Run 4**: (0.107247)\n",
    "This run included further incrementing the `num_layers` argument from 5 to 10. This adds additional further layers to capture more of the complex patterns in the financial dataset. \n",
    "\n",
    "_NOTE_: The `num_layers` does not seem to provide additional aid in this trainin task, with the side-effect of signifitcanlty increasing the inference time. As a result, we are making the decision to keep `num_layers = 3`.\n",
    "\n",
    "---\n",
    "\n",
    "**Run 5**: (0.108397)\n",
    "The `num_layers` argument has been reset to a value of 3, which returns our baseline back to _Run 2_. The `expansion_factor` has been increased from 3 to 4. This yeilded a slight decrease in validation loss, so potentially worth running a second experiment, but likely best to test patching instead.\n",
    "\n",
    "**Run 6** ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27812e8c-c0f6-45e3-a075-310929329460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing forecasting training on FULL Dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9000' max='902100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  9000/902100 1:00:15 < 99:41:08, 2.49 it/s, Epoch 0/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.653000</td>\n",
       "      <td>0.703126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.525900</td>\n",
       "      <td>0.664058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.760900</td>\n",
       "      <td>0.645361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.183400</td>\n",
       "      <td>0.631627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.740400</td>\n",
       "      <td>0.624281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.418200</td>\n",
       "      <td>0.615170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.210100</td>\n",
       "      <td>0.609595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.054600</td>\n",
       "      <td>0.604529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.946200</td>\n",
       "      <td>0.601396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.868100</td>\n",
       "      <td>0.596927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.813200</td>\n",
       "      <td>0.598150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.773500</td>\n",
       "      <td>0.598260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.739800</td>\n",
       "      <td>0.593433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.719900</td>\n",
       "      <td>0.594852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.705900</td>\n",
       "      <td>0.597168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.696900</td>\n",
       "      <td>0.596341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.674500</td>\n",
       "      <td>0.594650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.672800</td>\n",
       "      <td>0.597483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9000, training_loss=1.4531816779242621, metrics={'train_runtime': 3617.9149, 'train_samples_per_second': 15957.921, 'train_steps_per_second': 249.343, 'total_flos': 1.40385042432e+16, 'train_loss': 1.4531816779242621, 'epoch': 0.9976720984369803})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the run number\n",
    "run_num = \"snap_candle_8\"\n",
    "save_dir = f\"./checkpoints/run_{run_num}\"\n",
    "\n",
    "# Check if save_dir exists\n",
    "assert not os.path.exists(save_dir), \"Please update the run_num to avoid overwriting checkpoints!\"\n",
    "\n",
    "num_workers = 10  # p3.2xlarge instance has 12 vCPUs\n",
    "\n",
    "gradient_accumulation_steps = 1 # Number of batches between each backward pass\n",
    "batch_size = 64 # Size of each batches sent to GPU\n",
    "eval_batch_size = 256\n",
    "num_steps = 500\n",
    "\n",
    "# Calculations\n",
    "# =======================\n",
    "# effective_batch_size = batch_size * grad_accumulation_steps = 64 * 1 = 64\n",
    "# examples_per_evaluation = num_steps * effective_batch_size = 64 * 5,000 = 320,0000\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=f\"{save_dir}/output/\",\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=0.00001,\n",
    "    num_train_epochs=100,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=num_steps,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    eval_accumulation_steps=250,\n",
    "    dataloader_num_workers=num_workers,\n",
    "    report_to=\"tensorboard\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=num_steps,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=num_steps,\n",
    "    save_total_limit=3,\n",
    "    logging_dir=f\"{save_dir}/logs/\",  # Make sure to specify a logging directory\n",
    "    load_best_model_at_end=True,  # Load the best model when training ends\n",
    "    metric_for_best_model=\"eval_loss\",  # Metric to monitor for early stopping\n",
    "    greater_is_better=False,  # For loss\n",
    "    label_names=[\"future_values\"], # The names of the \"ground truth\" values to compare predictions against\n",
    ")\n",
    "\n",
    "# Create a new early stopping callback with faster convergence properties\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=5,  # Number of epochs with no improvement after which to stop\n",
    "    early_stopping_threshold=0.001,  # Minimum improvement required to consider as improvement\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    callbacks=[early_stopping_callback],\n",
    ")\n",
    "\n",
    "print(\"Doing forecasting training on FULL Dataset\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e06f3931-6b5f-450e-b17d-360ae3984e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='284' max='284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [284/284 00:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5191817879676819,\n",
       " 'eval_runtime': 50.6359,\n",
       " 'eval_samples_per_second': 1432.896,\n",
       " 'eval_steps_per_second': 5.609,\n",
       " 'epoch': 0.942245870746037}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fd01ef-5b6c-4bac-bc42-25052c17b45f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-kernel",
   "language": "python",
   "name": "venv-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
