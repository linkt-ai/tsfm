{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7478e0e2-b7af-4fd4-b44e-ca58e0c31b71",
   "metadata": {},
   "source": [
    "# MONGY: Training `PatchTSMixer` on Financial Candlestick Data\n",
    "## Direct forecasting example\n",
    "\n",
    "This notebooke demonstrates the usage of a `PatchTSMixer` model for a multivariate time series forecasting task. This notebook has a dependecy on HuggingFace [transformers](https://github.com/huggingface/transformers) repo. For details related to model architecture, refer to the [TSMixer paper](https://arxiv.org/abs/2306.09364)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f63ae353-96df-4380-89f6-1e6cebf684fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Third Party\n",
    "from transformers import (\n",
    "    EarlyStoppingCallback,\n",
    "    PatchTSMixerConfig,\n",
    "    PatchTSMixerForPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# First Party\n",
    "from tsfm_public.toolkit.dataset import ForecastDFDataset\n",
    "from tsfm_public.toolkit.time_series_preprocessor import TimeSeriesPreprocessor\n",
    "from tsfm_public.toolkit.util import select_by_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a826c4f3-1c6c-4088-b6af-f430f45fd380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27bb0d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.utils.data.dataloader\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4eb9be-c19f-448f-a4bd-c600e068633f",
   "metadata": {},
   "source": [
    "## Load and prepare datasets\n",
    "\n",
    "In the next cell, please adjust the following parameters to suit your application:\n",
    "- `dataset_path`: path to local .csv file, or web address to a csv file for the data of interest. Data is loaded with pandas, so anything supported by\n",
    "`pd.read_csv` is supported: (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html).\n",
    "- `timestamp_column`: column name containing timestamp information, use None if there is no such column\n",
    "- `id_columns`: List of column names specifying the IDs of different time series. If no ID column exists, use []\n",
    "- `forecast_columns`: List of columns to be modeled\n",
    "- `context_length`: The amount of historical data used as input to the model. Windows of the input time series data with length equal to\n",
    "context_length will be extracted from the input dataframe. In the case of a multi-time series dataset, the context windows will be created\n",
    "so that they are contained within a single time series (i.e., a single ID).\n",
    "- `forecast_horizon`: Number of time stamps to forecast in future.\n",
    "- `train_start_index`, `train_end_index`: the start and end indices in the loaded data which delineate the training data.\n",
    "- `valid_start_index`, `valid_end_index`: the start and end indices in the loaded data which delineate the validation data.\n",
    "- `test_start_index`, `test_end_index`: the start and end indices in the loaded data which delineate the test data.\n",
    "- `patch_length`: The patch length for the `PatchTSMixer` model. Recommended to have a value so that `context_length` is divisible by it.\n",
    "- `num_workers`: Number of dataloder workers in pytorch dataloader.\n",
    "- `batch_size`: Batch size. \n",
    "The data is first loaded into a Pandas dataframe and split into training, validation, and test parts. Then the pandas dataframes are converted\n",
    "to the appropriate torch dataset needed for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4c1e812-f2d6-4ccb-a79c-47879b562d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to setup our context, horizon, and patch size based on our task. We want to use\n",
    "# 4 hours of lookback to start, in order to predict the next 5 minutes of candles. Regarding\n",
    "# patch length, we know that we will want a larger patch size, so we will start with 64 as\n",
    "# a base case assumption\n",
    "context_length = 6 * 60 * 4  # This will give us 4 hours of lookback (6 candles per min * 60 min per hour)\n",
    "forecast_horizon = 6 * 20 # This will give us 20 minutes of predictions\n",
    "patch_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ba7a031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Dataset from the CSV file\n",
    "DATASET_PATH = \"/home/jack/data/10s-candles-2023.csv\"\n",
    "timestamp_col = 't'\n",
    "\n",
    "full_dataset = pd.read_csv(\n",
    "    DATASET_PATH,\n",
    "    parse_dates=[timestamp_col]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19197d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31160015, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>targ_o</th>\n",
       "      <th>targ_h</th>\n",
       "      <th>targ_l</th>\n",
       "      <th>targ_c</th>\n",
       "      <th>targ_v</th>\n",
       "      <th>ticker</th>\n",
       "      <th>market_state_MarketState.CLOSED</th>\n",
       "      <th>market_state_MarketState.EXTENDED</th>\n",
       "      <th>market_state_MarketState.OPEN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-03 09:00:00+00:00</td>\n",
       "      <td>130.28</td>\n",
       "      <td>130.95</td>\n",
       "      <td>130.28</td>\n",
       "      <td>130.95</td>\n",
       "      <td>4233.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-03 09:00:10+00:00</td>\n",
       "      <td>130.98</td>\n",
       "      <td>131.00</td>\n",
       "      <td>130.93</td>\n",
       "      <td>130.93</td>\n",
       "      <td>744.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-03 09:00:20+00:00</td>\n",
       "      <td>130.98</td>\n",
       "      <td>131.00</td>\n",
       "      <td>130.93</td>\n",
       "      <td>130.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-03 09:00:30+00:00</td>\n",
       "      <td>130.98</td>\n",
       "      <td>131.00</td>\n",
       "      <td>130.93</td>\n",
       "      <td>130.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-03 09:00:40+00:00</td>\n",
       "      <td>130.98</td>\n",
       "      <td>130.98</td>\n",
       "      <td>130.98</td>\n",
       "      <td>130.98</td>\n",
       "      <td>223.0</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          t  targ_o  targ_h  targ_l  targ_c  targ_v ticker  \\\n",
       "0 2023-01-03 09:00:00+00:00  130.28  130.95  130.28  130.95  4233.0   AAPL   \n",
       "1 2023-01-03 09:00:10+00:00  130.98  131.00  130.93  130.93   744.0   AAPL   \n",
       "2 2023-01-03 09:00:20+00:00  130.98  131.00  130.93  130.93     0.0   AAPL   \n",
       "3 2023-01-03 09:00:30+00:00  130.98  131.00  130.93  130.93     0.0   AAPL   \n",
       "4 2023-01-03 09:00:40+00:00  130.98  130.98  130.98  130.98   223.0   AAPL   \n",
       "\n",
       "   market_state_MarketState.CLOSED  market_state_MarketState.EXTENDED  \\\n",
       "0                                0                                  1   \n",
       "1                                0                                  1   \n",
       "2                                0                                  1   \n",
       "3                                0                                  1   \n",
       "4                                0                                  1   \n",
       "\n",
       "   market_state_MarketState.OPEN  \n",
       "0                              0  \n",
       "1                              0  \n",
       "2                              0  \n",
       "3                              0  \n",
       "4                              0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(full_dataset.shape)\n",
    "full_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4d7fc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to trim down the dataframe, to only include the AAPL data, and we will\n",
    "# additionally remove the market_state columns\n",
    "data = full_dataset.loc[full_dataset['ticker'] == 'AAPL']\n",
    "data = data.drop(columns=[\"ticker\", \"market_state_MarketState.CLOSED\", \"market_state_MarketState.OPEN\", \"market_state_MarketState.EXTENDED\"])\n",
    "\n",
    "data = data.ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a125a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t         0\n",
       "targ_o    0\n",
       "targ_h    0\n",
       "targ_l    0\n",
       "targ_c    0\n",
       "targ_v    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for NaN values\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "796d1e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we set the explicit train, validation, and test indicies, let's retrieve these\n",
    "# indicies by searching over the 't' column\n",
    "\n",
    "# Find the index of the row with the earliest timestamp on October 1, 2023\n",
    "mar_1_2023 = '2023-03-01'\n",
    "mar_1_2023_index = data[timestamp_col].searchsorted(pd.to_datetime(mar_1_2023).tz_localize('UTC'))\n",
    "\n",
    "mar_15_2023 = '2023-03-15'\n",
    "mar_15_2023_index = data[timestamp_col].searchsorted(pd.to_datetime(mar_15_2023).tz_localize('UTC'))\n",
    "\n",
    "apr_1_2023 = '2023-04-01'\n",
    "apr_1_2023_index = data[timestamp_col].searchsorted(pd.to_datetime(apr_1_2023).tz_localize('UTC'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19ca5a76-64d4-4f8d-92f5-67f76c1685fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_columns = [] # since we only have one ticker, this can remain empty for now\n",
    "forecast_columns = [\"targ_o\", \"targ_h\", \"targ_l\", \"targ_c\", \"targ_v\"]\n",
    "train_start_index = None  # None indicates beginning of dataset\n",
    "train_end_index = mar_1_2023_index\n",
    "\n",
    "# we shift the start of the validation period back by context length so that\n",
    "# the first validation timestamp is immediately following the training data\n",
    "valid_start_index = train_end_index - context_length\n",
    "# we will end the validation set at the same date that the test set begins\n",
    "valid_end_index = mar_15_2023_index\n",
    "\n",
    "# we shift the start of the test period back by context length so that\n",
    "# the first test timestamp is immediately following the validation data\n",
    "test_start_index = valid_end_index - context_length\n",
    "test_end_index = apr_1_2023_index # none indicates the end of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f28412d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeSeriesPreprocessor {\n",
       "  \"categorical_encoder\": null,\n",
       "  \"conditional_columns\": [],\n",
       "  \"context_length\": 64,\n",
       "  \"control_columns\": [],\n",
       "  \"encode_categorical\": true,\n",
       "  \"feature_extractor_type\": \"TimeSeriesPreprocessor\",\n",
       "  \"freq\": \"0 days 00:00:10\",\n",
       "  \"frequency_mapping\": {\n",
       "    \"10_minutes\": 3,\n",
       "    \"15_minutes\": 4,\n",
       "    \"half_hourly\": 1,\n",
       "    \"hourly\": 2,\n",
       "    \"oov\": 0\n",
       "  },\n",
       "  \"id_columns\": [],\n",
       "  \"observable_columns\": [],\n",
       "  \"prediction_length\": null,\n",
       "  \"processor_class\": \"TimeSeriesPreprocessor\",\n",
       "  \"scaler_dict\": {},\n",
       "  \"scaler_type\": \"standard\",\n",
       "  \"scaling\": true,\n",
       "  \"scaling_id_columns\": [],\n",
       "  \"static_categorical_columns\": [],\n",
       "  \"target_columns\": [\n",
       "    \"targ_o\",\n",
       "    \"targ_h\",\n",
       "    \"targ_l\",\n",
       "    \"targ_c\",\n",
       "    \"targ_v\"\n",
       "  ],\n",
       "  \"target_scaler_dict\": {\n",
       "    \"0\": {\n",
       "      \"copy\": true,\n",
       "      \"feature_names_in_\": [\n",
       "        \"targ_o\",\n",
       "        \"targ_h\",\n",
       "        \"targ_l\",\n",
       "        \"targ_c\",\n",
       "        \"targ_v\"\n",
       "      ],\n",
       "      \"mean_\": [\n",
       "        143.1905872291718,\n",
       "        143.2082004746137,\n",
       "        143.1806988829613,\n",
       "        143.19542647841553,\n",
       "        4754.663292044805\n",
       "      ],\n",
       "      \"n_features_in_\": 5,\n",
       "      \"n_samples_seen_\": 489240,\n",
       "      \"scale_\": [\n",
       "        8.94912418508366,\n",
       "        8.950349393358659,\n",
       "        8.951007202632697,\n",
       "        8.95608692929087,\n",
       "        22975.300052826293\n",
       "      ],\n",
       "      \"var_\": [\n",
       "        80.08682368004926,\n",
       "        80.10875426319572,\n",
       "        80.12052994158243,\n",
       "        80.21149308501475,\n",
       "        527864412.51739985\n",
       "      ],\n",
       "      \"with_mean\": true,\n",
       "      \"with_std\": true\n",
       "    }\n",
       "  },\n",
       "  \"time_series_task\": \"forecasting\",\n",
       "  \"timestamp_column\": \"t\"\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_data = select_by_index(\n",
    "    data,\n",
    "    id_columns=id_columns,\n",
    "    start_index=train_start_index,\n",
    "    end_index=train_end_index,\n",
    ")\n",
    "valid_data = select_by_index(\n",
    "    data,\n",
    "    id_columns=id_columns,\n",
    "    start_index=valid_start_index,\n",
    "    end_index=valid_end_index,\n",
    ")\n",
    "test_data = select_by_index(\n",
    "    data,\n",
    "    id_columns=id_columns,\n",
    "    start_index=test_start_index,\n",
    "    end_index=test_end_index,\n",
    ")\n",
    "\n",
    "tsp = TimeSeriesPreprocessor(\n",
    "    timestamp_column=timestamp_col,\n",
    "    id_columns=id_columns,\n",
    "    target_columns=forecast_columns,\n",
    "    scaling=True,\n",
    ")\n",
    "tsp.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "678d849d-41fc-450d-a855-1dde27179b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ForecastDFDataset(\n",
    "    tsp.preprocess(train_data),\n",
    "    id_columns=id_columns,\n",
    "    target_columns=forecast_columns,\n",
    "    context_length=context_length,\n",
    "    prediction_length=forecast_horizon,\n",
    ")\n",
    "valid_dataset = ForecastDFDataset(\n",
    "    tsp.preprocess(valid_data),\n",
    "    id_columns=id_columns,\n",
    "    target_columns=forecast_columns,\n",
    "    context_length=context_length,\n",
    "    prediction_length=forecast_horizon,\n",
    ")\n",
    "test_dataset = ForecastDFDataset(\n",
    "    tsp.preprocess(test_data),\n",
    "    id_columns=id_columns,\n",
    "    target_columns=forecast_columns,\n",
    "    context_length=context_length,\n",
    "    prediction_length=forecast_horizon,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19456329-1293-45bf-99c7-e5ccf0534846",
   "metadata": {},
   "source": [
    "## Training `PatchTSMixer` From Scratch\n",
    "\n",
    "Adjust the following model parameters according to need.\n",
    "- `d_model` (`int`, *optional*, defaults to 8):\n",
    "    Hidden dimension of the model. Recommended to set it as a multiple of patch_length (i.e. 2-8X of\n",
    "    patch_len). Larger value indicates more complex model.\n",
    "- `expansion_factor` (`int`, *optional*, defaults to 2):\n",
    "    Expansion factor to use inside MLP. Recommended range is 2-5. Larger value indicates more complex model.\n",
    "- `num_layers` (`int`, *optional*, defaults to 3):\n",
    "    Number of layers to use. Recommended range is 3-15. Larger value indicates more complex model.\n",
    "- `mode`: (`str`, either to 'common_channel' or `mix_channel`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "226b904e-1ab2-478b-98b4-ce99bc23f1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PatchTSMixerConfig(\n",
    "    context_length=context_length,\n",
    "    prediction_length=forecast_horizon,\n",
    "    patch_length=patch_length,\n",
    "    num_input_channels=len(forecast_columns),\n",
    "    patch_stride=int(patch_length / 2),\n",
    "    d_model=2 * patch_length,\n",
    "    num_layers=3,\n",
    "    expansion_factor=4,\n",
    "    dropout=0.5,\n",
    "    head_dropout=0.7,\n",
    "    mode=\"mix_channel\",\n",
    "    scaling=\"std\",\n",
    ")\n",
    "model = PatchTSMixerForPrediction(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fae7e0c",
   "metadata": {},
   "source": [
    "# Training Run Summaries\n",
    "\n",
    "**Run 1**: (N/A)\n",
    "This run used the full year of data, and was used as a baseline to establish that the `mix_channel` mode is more effective for our task. Additionally, all subsequent runs have been updated, to instead use only the first three months of data as training data. Thus, while the loss for this run is lower, it is not indicaitve of the paramters being a better fit, just a result of having a larger dataset.\n",
    "\n",
    "**Run 2** (0.108476):\n",
    "This run was the first in which only the first two months of data was used as a training set. March was then split in half to form the validation and test sets. Additionally, the context window was expanded, to include the last four hours of data. While this wasn't explicitly compared against a shorter context window with the same dataset, the results of the paper provide an incredibly strong suggestions towards this approach yielding more effective performance.\n",
    "\n",
    "**Run 3** (0.108230):\n",
    "This run included involved increasing the `num_layers` argument from 3 to 5. This adds additional layers to the model, giving it more of an ability to percieve complex patterns in the financial data. This results in a larger model, but hopefully, will allow the model to better understand the nuances of the highly complex financial data it is being trained on.\n",
    "\n",
    "**Run 4**: (0.107247)\n",
    "This run included further incrementing the `num_layers` argument from 5 to 10. This adds additional further layers to capture more of the complex patterns in the financial dataset. \n",
    "\n",
    "_NOTE_: The `num_layers` does not seem to provide additional aid in this trainin task, with the side-effect of signifitcanlty increasing the inference time. As a result, we are making the decision to keep `num_layers = 3`.\n",
    "\n",
    "---\n",
    "\n",
    "**Run 5**: (0.108397)\n",
    "The `num_layers` argument has been reset to a value of 3, which returns our baseline back to _Run 2_. The `expansion_factor` has been increased from 3 to 4. This yeilded a slight decrease in validation loss, so potentially worth running a second experiment, but likely best to test patching instead.\n",
    "\n",
    "**Run 6** ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27812e8c-c0f6-45e3-a075-310929329460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing forecasting training on AAPL Dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='47600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  350/47600 08:37 < 19:30:33, 0.67 it/s, Epoch 0/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.228200</td>\n",
       "      <td>0.127111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.227700</td>\n",
       "      <td>0.124301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.193300</td>\n",
       "      <td>0.122547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.197600</td>\n",
       "      <td>0.121350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.180100</td>\n",
       "      <td>0.120407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.186800</td>\n",
       "      <td>0.119624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.197700</td>\n",
       "      <td>0.119097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=350, training_loss=0.20163015638078963, metrics={'train_runtime': 518.6755, 'train_samples_per_second': 94024.294, 'train_steps_per_second': 91.772, 'total_flos': 1.83485288448e+16, 'train_loss': 0.20163015638078963, 'epoch': 0.7345225603357818})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the run number\n",
    "run_num = 8\n",
    "save_dir = f\"./checkpoints/run_{run_num}\"\n",
    "\n",
    "# Check if save_dir exists\n",
    "assert not os.path.exists(save_dir), \"Please update the run_num to avoid overwriting checkpoints!\"\n",
    "\n",
    "num_workers = 8  # g4dn instance has 4 vCPUs\n",
    "batch_size = 256 # Size of each batches sent to GPU\n",
    "num_steps = 100\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=f\"{save_dir}/output/\",\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=0.0001,\n",
    "    num_train_epochs=100,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=num_steps,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=int(batch_size // 2), # Cut batch size down for eval\n",
    "    gradient_accumulation_steps=4,\n",
    "    dataloader_num_workers=num_workers,\n",
    "    report_to=\"tensorboard\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=num_steps,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=num_steps,\n",
    "    save_total_limit=3,\n",
    "    logging_dir=f\"{save_dir}/logs/\",  # Make sure to specify a logging directory\n",
    "    load_best_model_at_end=True,  # Load the best model when training ends\n",
    "    metric_for_best_model=\"eval_loss\",  # Metric to monitor for early stopping\n",
    "    greater_is_better=False,  # For loss\n",
    "    label_names=[\"future_values\"],\n",
    ")\n",
    "\n",
    "# Create a new early stopping callback with faster convergence properties\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=5,  # Number of epochs with no improvement after which to stop\n",
    "    early_stopping_threshold=0.001,  # Minimum improvement required to consider as improvement\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    callbacks=[early_stopping_callback],\n",
    ")\n",
    "\n",
    "print(\"Doing forecasting training on AAPL Dataset\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e06f3931-6b5f-450e-b17d-360ae3984e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jack/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3041' max='3041' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3041/3041 00:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2793983221054077,\n",
       " 'eval_runtime': 32.6588,\n",
       " 'eval_samples_per_second': 11915.05,\n",
       " 'eval_steps_per_second': 93.114,\n",
       " 'epoch': 4.0}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e206fac-39ef-4101-8d56-d8fd2c76b72f",
   "metadata": {},
   "source": [
    "## If we want to train from scratch for a few specific forecast channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e1a42c9-a3dd-4a54-ab20-bdd10a1903ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_channel_indices = [\n",
    "    -4,\n",
    "    -1,\n",
    "]  # add the channel indices (i.e., the column number) for which the model should forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7c7dbbc-2952-4a8b-ac4d-a05257c7afa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PatchTSMixerConfig(\n",
    "    context_length=context_length,\n",
    "    prediction_length=forecast_horizon,\n",
    "    patch_length=patch_length,\n",
    "    num_input_channels=len(forecast_columns),\n",
    "    patch_stride=patch_length,\n",
    "    d_model=48,\n",
    "    num_layers=3,\n",
    "    expansion_factor=3,\n",
    "    dropout=0.5,\n",
    "    head_dropout=0.7,\n",
    "    mode=\"common_channel\",\n",
    "    scaling=\"std\",\n",
    "    prediction_channel_indices=forecast_channel_indices,\n",
    ")\n",
    "model = PatchTSMixerForPrediction(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63aa0a38-c56f-4abd-92a6-9e9e5daadf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Doing forecasting training on Etth1/train\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4284' max='25200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4284/25200 00:59 < 04:51, 71.82 it/s, Epoch 17/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.275300</td>\n",
       "      <td>0.496316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.231200</td>\n",
       "      <td>0.485542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.218200</td>\n",
       "      <td>0.478069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.209900</td>\n",
       "      <td>0.470516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.206400</td>\n",
       "      <td>0.477010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.202600</td>\n",
       "      <td>0.474555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.200600</td>\n",
       "      <td>0.474283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.198300</td>\n",
       "      <td>0.472296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.196000</td>\n",
       "      <td>0.464579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.194800</td>\n",
       "      <td>0.467563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.193500</td>\n",
       "      <td>0.467156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.192500</td>\n",
       "      <td>0.456910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.190300</td>\n",
       "      <td>0.464192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.189500</td>\n",
       "      <td>0.465797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.188200</td>\n",
       "      <td>0.468837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.187800</td>\n",
       "      <td>0.471794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.186300</td>\n",
       "      <td>0.473285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4284, training_loss=0.20359806520264356, metrics={'train_runtime': 60.2954, 'train_samples_per_second': 13322.735, 'train_steps_per_second': 417.942, 'total_flos': 1268896459751424.0, 'train_loss': 0.20359806520264356, 'epoch': 17.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    callbacks=[early_stopping_callback],\n",
    ")\n",
    "\n",
    "print(\"\\n\\nDoing forecasting training on Etth1/train\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec5de472-7e2e-4bb2-8cdb-6371d0a33ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.1160622164607048,\n",
       " 'eval_runtime': 0.7379,\n",
       " 'eval_samples_per_second': 3774.245,\n",
       " 'eval_steps_per_second': 119.258,\n",
       " 'epoch': 17.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2e8f0a-8367-4c10-84bd-a72e8f21ccc4",
   "metadata": {},
   "source": [
    "#### Sanity check: Compute number of forecasting channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1cdf078-10e8-4788-b7e4-3d1640ab8f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4db1b83f-7381-4fe6-97e4-42ee210ab71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2785, 96, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.predictions[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
