{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Candle Dataset Cleaning\n",
    "\n",
    "After running the first training job on the full dataset, we can see that despite the training loss decreasing, the validation loss is remaining fairly constant. As a potential remedy, we are going to try removing all of the rows from a closed market from the dataset. This will have a few advantages:\n",
    "1. It will drasitcally reduce the size of the data by approximately 55 - 60%, by keeping only the examples that are relevant to the task we need to perform. \n",
    "2. It will ensure that the loss metrics are mearusing the models performnace on market data where trading is occuring. Currently, the loss function is likely dilluted, as so much of the data is just a flat time-series, that the model is likely performing perfectly on these examples, causing the loss metric to be diluted, and likely not allowing a high enough gradient to be built up for the backward pass.\n",
    "\n",
    "## Plan for Cleaning Data \n",
    "\n",
    "1. Remove any rows where the market is closed. We will never be using this in the actual model.\n",
    "2. This should give us a a couple hundred time-series per ticker. We want to create a time-series ID for each continuous time series in the dataset. This will be used as a column ID.\n",
    "3. Then iterate over each ticker. For each ticker:\n",
    " - Use our date indicices, to create a train, validation, and test set for that ticker. Be sure that the date the sets do not slice any time-series (e.g. each of the 3 sets should have a unique set of time-series IDs)\n",
    " - Append eacah of the three sets to a master train, validation, and test set respectively. \n",
    " - These steps will replace the current `select_by_index` usage\n",
    "4. Train the `TimeSeriesPreprocessor` on the train set\n",
    "5. Create the train, validation, and test datasets, using the trained preprocessor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Third Party\n",
    "from transformers import (\n",
    "    EarlyStoppingCallback,\n",
    "    PatchTSMixerConfig,\n",
    "    PatchTSMixerForPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First Party\n",
    "from tsfm_public.toolkit.dataset import ForecastDFDataset\n",
    "from tsfm_public.toolkit.time_series_preprocessor import TimeSeriesPreprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing\n",
    "\n",
    "We want to meticulously craft our examples for each trading day. Our primary focus here is to ensure that the model only is ever asked to forecast into live market data. We need it to understand this type of forecasting, and not be thrown off at all by examples from extended hours trading.\n",
    "\n",
    "There are a few general steps we must take:\n",
    "1. Ensure that all the data from the closed market is removed from the dataset, as this data will never be used in context or in forecasting.\n",
    "2. Ensure that the timestamps are localized to America/New_York, so that we can get accurate date_strings to use for identifying each trading day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Dataset from the CSV file\n",
    "DATA_DIR = \"/home/ubuntu/verb-workspace/data\" # set this accordingly to the location of the data\n",
    "\n",
    "TRAIN_DATASET = f\"{DATA_DIR}/1min-candles-train-w-CANDLES.csv\"\n",
    "VALID_DATASET = f\"{DATA_DIR}/1min-candles-valid-w-CANDLES.csv\"\n",
    "TEST_DATASET = f\"{DATA_DIR}/1min-candles-test-w-CANDLES.csv\"\n",
    "\n",
    "timestamp_col = 't'\n",
    "\n",
    "train_data = pd.read_csv(\n",
    "    TRAIN_DATASET,\n",
    "    parse_dates=[timestamp_col]\n",
    ")\n",
    "\n",
    "valid_data = pd.read_csv(\n",
    "    VALID_DATASET,\n",
    "    parse_dates=[timestamp_col]\n",
    ")\n",
    "\n",
    "test_data = pd.read_csv(\n",
    "    TEST_DATASET,\n",
    "    parse_dates=[timestamp_col]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 937440 entries, 0 to 937439\n",
      "Data columns (total 13 columns):\n",
      " #   Column                Non-Null Count   Dtype  \n",
      "---  ------                --------------   -----  \n",
      " 0   ticker                937440 non-null  object \n",
      " 1   date_string           937440 non-null  object \n",
      " 2   t                     937440 non-null  object \n",
      " 3   targ_o                937440 non-null  float64\n",
      " 4   targ_h                937440 non-null  float64\n",
      " 5   targ_l                937440 non-null  float64\n",
      " 6   targ_c                937440 non-null  float64\n",
      " 7   targ_v                937440 non-null  float64\n",
      " 8   targ_vwap             937440 non-null  float64\n",
      " 9   targ_red              937440 non-null  int64  \n",
      " 10  targ_green            937440 non-null  int64  \n",
      " 11  cont_market_open      937440 non-null  int64  \n",
      " 12  cont_market_extended  937440 non-null  int64  \n",
      "dtypes: float64(6), int64(4), object(3)\n",
      "memory usage: 93.0+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into a morning set, maybe the first 90 minutes of the day\n",
    "\n",
    "# Convert each df to have a timestamp val as ther 't' column\n",
    "train_data['t'] = pd.to_datetime(train_data['t'], utc=True).dt.tz_convert('America/New_York')\n",
    "valid_data['t'] = pd.to_datetime(valid_data['t'], utc=True).dt.tz_convert('America/New_York')\n",
    "test_data['t'] = pd.to_datetime(test_data['t'], utc=True).dt.tz_convert('America/New_York')\n",
    "\n",
    "# Create a time object representing 11 AM\n",
    "eleven_am = pd.to_datetime('11:00:00', format='%H:%M:%S').time()\n",
    "\n",
    "# Filter each dataset to keep rows where 't' is before or equal to 11 AM\n",
    "morning_train_data = train_data[train_data['t'].dt.time <= eleven_am]\n",
    "morning_valid_data = valid_data[valid_data['t'].dt.time <= eleven_am]\n",
    "morning_test_data = test_data[test_data['t'].dt.time <= eleven_am]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert morning_train_data.isna().sum().sum() == 0\n",
    "assert morning_valid_data.isna().sum().sum() == 0\n",
    "assert morning_test_data.isna().sum().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET_1_MIN = f\"{DATA_DIR}/1min-candles-train-MORNING.csv\"\n",
    "VALID_DATASET_1_MIN = f\"{DATA_DIR}/1min-candles-valid-MORNING.csv\"\n",
    "TEST_DATASET_1_MIN = f\"{DATA_DIR}/1min-candles-test-MORNING.csv\"\n",
    "\n",
    "morning_train_data.to_csv(TRAIN_DATASET_1_MIN, index=False)\n",
    "morning_valid_data.to_csv(VALID_DATASET_1_MIN, index=False)\n",
    "morning_test_data.to_csv(TEST_DATASET_1_MIN, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we want to get the afternoon split, which involves taking everything until 2:30 PM\n",
    "\n",
    "# Create a time object representing 2:30 PM\n",
    "two_thirty_pm = pd.to_datetime('14:30:00', format='%H:%M:%S').time()\n",
    "\n",
    "# Create a time object representing 9 AM (4 hours leading up to 11 AM)\n",
    "nine_am = pd.to_datetime('07:00:00', format='%H:%M:%S').time()\n",
    "\n",
    "\n",
    "# Filter each dataset to keep rows where 't' is between 10:30 AM and 2:30 PM\n",
    "day_train_data = train_data[(train_data['t'].dt.time >= nine_am) & (train_data['t'].dt.time <= two_thirty_pm)]\n",
    "day_valid_data = valid_data[(valid_data['t'].dt.time >= nine_am) & (valid_data['t'].dt.time <= two_thirty_pm)]\n",
    "day_test_data = test_data[(test_data['t'].dt.time >= nine_am) & (test_data['t'].dt.time <= two_thirty_pm)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET_1_MIN = f\"{DATA_DIR}/1min-candles-train-DAY.csv\"\n",
    "VALID_DATASET_1_MIN = f\"{DATA_DIR}/1min-candles-valid-DAY.csv\"\n",
    "TEST_DATASET_1_MIN = f\"{DATA_DIR}/1min-candles-test-DAY.csv\"\n",
    "\n",
    "day_train_data.to_csv(TRAIN_DATASET_1_MIN, index=False)\n",
    "day_valid_data.to_csv(VALID_DATASET_1_MIN, index=False)\n",
    "day_test_data.to_csv(TEST_DATASET_1_MIN, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we want to get the afternoon split, which involves taking everything after 2:30 PM\n",
    "\n",
    "# Create a time object representing 10:30 AM (4 hours leading up to 2:30 PM)\n",
    "ten_thirty_am = pd.to_datetime('10:30:00', format='%H:%M:%S').time()\n",
    "\n",
    "\n",
    "# Filter each dataset to keep rows where 't' is between 10:30 AM and 2:30 PM\n",
    "afternoon_train_data = train_data[train_data['t'].dt.time >= ten_thirty_am]\n",
    "afternoon_valid_data = valid_data[valid_data['t'].dt.time >= ten_thirty_am]\n",
    "afternoon_test_data = test_data[test_data['t'].dt.time >= ten_thirty_am]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET_1_MIN = f\"{DATA_DIR}/1min-candles-train-AFTERNOON.csv\"\n",
    "VALID_DATASET_1_MIN = f\"{DATA_DIR}/1min-candles-valid-AFTERNOON.csv\"\n",
    "TEST_DATASET_1_MIN = f\"{DATA_DIR}/1min-candles-test-AFTERNOON.csv\"\n",
    "\n",
    "afternoon_train_data.to_csv(TRAIN_DATASET_1_MIN, index=False)\n",
    "afternoon_valid_data.to_csv(VALID_DATASET_1_MIN, index=False)\n",
    "afternoon_test_data.to_csv(TEST_DATASET_1_MIN, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-kernel",
   "language": "python",
   "name": "venv-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
