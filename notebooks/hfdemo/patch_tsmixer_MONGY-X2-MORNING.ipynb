{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7478e0e2-b7af-4fd4-b44e-ca58e0c31b71",
   "metadata": {},
   "source": [
    "# MONGY: Training `PatchTSMixer` on Financial Candlestick Data\n",
    "## Direct forecasting example\n",
    "\n",
    "This notebooke demonstrates the usage of a `PatchTSMixer` model for a multivariate time series forecasting task. This notebook has a dependecy on HuggingFace [transformers](https://github.com/huggingface/transformers) repo. For details related to model architecture, refer to the [TSMixer paper](https://arxiv.org/abs/2306.09364)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f63ae353-96df-4380-89f6-1e6cebf684fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Third Party\n",
    "from transformers import (\n",
    "    EarlyStoppingCallback,\n",
    "    PatchTSMixerConfig,\n",
    "    PatchTSMixerForPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# First Party\n",
    "from tsfm_public.toolkit.dataset import ForecastDFDataset\n",
    "from tsfm_public.toolkit.time_series_preprocessor import TimeSeriesPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a826c4f3-1c6c-4088-b6af-f430f45fd380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27bb0d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.utils.data.dataloader\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4eb9be-c19f-448f-a4bd-c600e068633f",
   "metadata": {},
   "source": [
    "## Load and prepare datasets\n",
    "\n",
    "In the next cell, please adjust the following parameters to suit your application:\n",
    "- `dataset_path`: path to local .csv file, or web address to a csv file for the data of interest. Data is loaded with pandas, so anything supported by\n",
    "`pd.read_csv` is supported: (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html).\n",
    "- `timestamp_column`: column name containing timestamp information, use None if there is no such column\n",
    "- `id_columns`: List of column names specifying the IDs of different time series. If no ID column exists, use []\n",
    "- `forecast_columns`: List of columns to be modeled\n",
    "- `context_length`: The amount of historical data used as input to the model. Windows of the input time series data with length equal to\n",
    "context_length will be extracted from the input dataframe. In the case of a multi-time series dataset, the context windows will be created\n",
    "so that they are contained within a single time series (i.e., a single ID).\n",
    "- `forecast_horizon`: Number of time stamps to forecast in future.\n",
    "- `train_start_index`, `train_end_index`: the start and end indices in the loaded data which delineate the training data.\n",
    "- `valid_start_index`, `valid_end_index`: the start and end indices in the loaded data which delineate the validation data.\n",
    "- `test_start_index`, `test_end_index`: the start and end indices in the loaded data which delineate the test data.\n",
    "- `patch_length`: The patch length for the `PatchTSMixer` model. Recommended to have a value so that `context_length` is divisible by it.\n",
    "- `num_workers`: Number of dataloder workers in pytorch dataloader.\n",
    "- `batch_size`: Batch size. \n",
    "The data is first loaded into a Pandas dataframe and split into training, validation, and test parts. Then the pandas dataframes are converted\n",
    "to the appropriate torch dataset needed for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4c1e812-f2d6-4ccb-a79c-47879b562d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to setup our context, horizon, and patch size based on our task. We want to use\n",
    "# 4 hours of lookback to start, in order to predict the next 5 minutes of candles. Regarding\n",
    "# patch length, we know that we will want a larger patch size, so we will start with 64 as\n",
    "# a base case assumption\n",
    "context_length = 60 * 4  # This will give us 4 hours of lookback (4 hours * 60 min per hour)\n",
    "forecast_horizon = 3 # This will give us 3 minutes of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ba7a031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Dataset from the CSV file\n",
    "DATA_DIR = \"/home/ubuntu/verb-workspace/data\"\n",
    "\n",
    "TRAIN_DATASET = f\"{DATA_DIR}/1min-candles-train-MORNING.csv\"\n",
    "VALID_DATASET = f\"{DATA_DIR}/1min-candles-valid-MORNING.csv\"\n",
    "TEST_DATASET = f\"{DATA_DIR}/1min-candles-test-MORNING.csv\"\n",
    "\n",
    "timestamp_col = 't'\n",
    "\n",
    "train_data = pd.read_csv(\n",
    "    TRAIN_DATASET,\n",
    "    parse_dates=[timestamp_col]\n",
    ")\n",
    "\n",
    "valid_data = pd.read_csv(\n",
    "    VALID_DATASET,\n",
    "    parse_dates=[timestamp_col]\n",
    ")\n",
    "\n",
    "test_data = pd.read_csv(\n",
    "    TEST_DATASET,\n",
    "    parse_dates=[timestamp_col]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a125a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values\n",
    "assert sum(train_data.isna().sum().to_list()) == 0\n",
    "assert sum(valid_data.isna().sum().to_list()) == 0\n",
    "assert sum(test_data.isna().sum().to_list()) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94bd97e3-4fbc-44b0-b464-a96eaa50c3f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date_string</th>\n",
       "      <th>t</th>\n",
       "      <th>targ_o</th>\n",
       "      <th>targ_h</th>\n",
       "      <th>targ_l</th>\n",
       "      <th>targ_c</th>\n",
       "      <th>targ_v</th>\n",
       "      <th>targ_vwap</th>\n",
       "      <th>targ_red</th>\n",
       "      <th>targ_green</th>\n",
       "      <th>cont_market_open</th>\n",
       "      <th>cont_market_extended</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>2023-01-03 05:30:00-05:00</td>\n",
       "      <td>130.80</td>\n",
       "      <td>130.80</td>\n",
       "      <td>130.8000</td>\n",
       "      <td>130.800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>2023-01-03 05:31:00-05:00</td>\n",
       "      <td>130.80</td>\n",
       "      <td>130.80</td>\n",
       "      <td>130.8000</td>\n",
       "      <td>130.800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>2023-01-03 05:32:00-05:00</td>\n",
       "      <td>130.80</td>\n",
       "      <td>130.80</td>\n",
       "      <td>130.8000</td>\n",
       "      <td>130.800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>2023-01-03 05:33:00-05:00</td>\n",
       "      <td>130.80</td>\n",
       "      <td>130.80</td>\n",
       "      <td>130.8000</td>\n",
       "      <td>130.800</td>\n",
       "      <td>235.0</td>\n",
       "      <td>130.8009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>2023-01-03 05:34:00-05:00</td>\n",
       "      <td>130.80</td>\n",
       "      <td>130.80</td>\n",
       "      <td>130.8000</td>\n",
       "      <td>130.800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492523</th>\n",
       "      <td>V</td>\n",
       "      <td>2023-11-17</td>\n",
       "      <td>2023-11-17 10:56:00-05:00</td>\n",
       "      <td>249.39</td>\n",
       "      <td>249.41</td>\n",
       "      <td>249.3400</td>\n",
       "      <td>249.390</td>\n",
       "      <td>10688.0</td>\n",
       "      <td>249.3708</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492524</th>\n",
       "      <td>V</td>\n",
       "      <td>2023-11-17</td>\n",
       "      <td>2023-11-17 10:57:00-05:00</td>\n",
       "      <td>249.43</td>\n",
       "      <td>249.43</td>\n",
       "      <td>249.3400</td>\n",
       "      <td>249.350</td>\n",
       "      <td>9789.0</td>\n",
       "      <td>249.3913</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492525</th>\n",
       "      <td>V</td>\n",
       "      <td>2023-11-17</td>\n",
       "      <td>2023-11-17 10:58:00-05:00</td>\n",
       "      <td>249.33</td>\n",
       "      <td>249.36</td>\n",
       "      <td>249.2101</td>\n",
       "      <td>249.270</td>\n",
       "      <td>13669.0</td>\n",
       "      <td>249.2788</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492526</th>\n",
       "      <td>V</td>\n",
       "      <td>2023-11-17</td>\n",
       "      <td>2023-11-17 10:59:00-05:00</td>\n",
       "      <td>249.30</td>\n",
       "      <td>249.30</td>\n",
       "      <td>249.1700</td>\n",
       "      <td>249.170</td>\n",
       "      <td>9620.0</td>\n",
       "      <td>249.2371</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492527</th>\n",
       "      <td>V</td>\n",
       "      <td>2023-11-17</td>\n",
       "      <td>2023-11-17 11:00:00-05:00</td>\n",
       "      <td>249.16</td>\n",
       "      <td>249.26</td>\n",
       "      <td>249.1400</td>\n",
       "      <td>249.255</td>\n",
       "      <td>8746.0</td>\n",
       "      <td>249.2048</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>492528 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ticker date_string                          t  targ_o  targ_h  \\\n",
       "0        AAPL  2023-01-03  2023-01-03 05:30:00-05:00  130.80  130.80   \n",
       "1        AAPL  2023-01-03  2023-01-03 05:31:00-05:00  130.80  130.80   \n",
       "2        AAPL  2023-01-03  2023-01-03 05:32:00-05:00  130.80  130.80   \n",
       "3        AAPL  2023-01-03  2023-01-03 05:33:00-05:00  130.80  130.80   \n",
       "4        AAPL  2023-01-03  2023-01-03 05:34:00-05:00  130.80  130.80   \n",
       "...       ...         ...                        ...     ...     ...   \n",
       "492523      V  2023-11-17  2023-11-17 10:56:00-05:00  249.39  249.41   \n",
       "492524      V  2023-11-17  2023-11-17 10:57:00-05:00  249.43  249.43   \n",
       "492525      V  2023-11-17  2023-11-17 10:58:00-05:00  249.33  249.36   \n",
       "492526      V  2023-11-17  2023-11-17 10:59:00-05:00  249.30  249.30   \n",
       "492527      V  2023-11-17  2023-11-17 11:00:00-05:00  249.16  249.26   \n",
       "\n",
       "          targ_l   targ_c   targ_v  targ_vwap  targ_red  targ_green  \\\n",
       "0       130.8000  130.800      0.0     0.0000         0           0   \n",
       "1       130.8000  130.800      0.0     0.0000         0           0   \n",
       "2       130.8000  130.800      0.0     0.0000         0           0   \n",
       "3       130.8000  130.800    235.0   130.8009         0           0   \n",
       "4       130.8000  130.800      0.0     0.0000         0           0   \n",
       "...          ...      ...      ...        ...       ...         ...   \n",
       "492523  249.3400  249.390  10688.0   249.3708         0           0   \n",
       "492524  249.3400  249.350   9789.0   249.3913         1           0   \n",
       "492525  249.2101  249.270  13669.0   249.2788         1           0   \n",
       "492526  249.1700  249.170   9620.0   249.2371         1           0   \n",
       "492527  249.1400  249.255   8746.0   249.2048         0           1   \n",
       "\n",
       "        cont_market_open  cont_market_extended  \n",
       "0                      0                     1  \n",
       "1                      0                     1  \n",
       "2                      0                     1  \n",
       "3                      0                     1  \n",
       "4                      0                     1  \n",
       "...                  ...                   ...  \n",
       "492523                 1                     0  \n",
       "492524                 1                     0  \n",
       "492525                 1                     0  \n",
       "492526                 1                     0  \n",
       "492527                 1                     0  \n",
       "\n",
       "[492528 rows x 13 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f28412d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Train\n",
      "Done Valid\n",
      "Done Test\n"
     ]
    }
   ],
   "source": [
    "\n",
    "id_columns = ['ticker', 'date_string']\n",
    "forecast_columns = ['targ_o', 'targ_c', 'targ_h', 'targ_l', 'targ_v', 'targ_vwap', 'targ_red', 'targ_green']\n",
    "control_columns = ['cont_market_open', 'cont_market_extended']\n",
    "\n",
    "train_tsp = TimeSeriesPreprocessor(\n",
    "    timestamp_column=timestamp_col,\n",
    "    id_columns=id_columns,\n",
    "    target_columns=forecast_columns,\n",
    "    control_columns=control_columns,\n",
    "    scaling=True,\n",
    ")\n",
    "train_tsp.train(train_data)\n",
    "print(\"Done Train\")\n",
    "\n",
    "valid_tsp = TimeSeriesPreprocessor(\n",
    "    timestamp_column=timestamp_col,\n",
    "    id_columns=id_columns,\n",
    "    target_columns=forecast_columns,\n",
    "    control_columns=control_columns,\n",
    "    scaling=True,\n",
    ")\n",
    "valid_tsp.train(valid_data)\n",
    "print(\"Done Valid\")\n",
    "\n",
    "test_tsp = TimeSeriesPreprocessor(\n",
    "    timestamp_column=timestamp_col,\n",
    "    id_columns=id_columns,\n",
    "    target_columns=forecast_columns,\n",
    "    control_columns=control_columns,\n",
    "    scaling=True,\n",
    ")\n",
    "test_tsp.train(test_data)\n",
    "print(\"Done Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "678d849d-41fc-450d-a855-1dde27179b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ForecastDFDataset(\n",
    "    train_tsp.preprocess(train_data),\n",
    "    id_columns=id_columns,\n",
    "    target_columns=forecast_columns,\n",
    "    control_columns=control_columns,\n",
    "    context_length=context_length,\n",
    "    prediction_length=forecast_horizon,\n",
    ")\n",
    "valid_dataset = ForecastDFDataset(\n",
    "    valid_tsp.preprocess(valid_data),\n",
    "    id_columns=id_columns,\n",
    "    target_columns=forecast_columns,\n",
    "    control_columns=control_columns,\n",
    "    context_length=context_length,\n",
    "    prediction_length=forecast_horizon,\n",
    ")\n",
    "test_dataset = ForecastDFDataset(\n",
    "    test_tsp.preprocess(test_data),\n",
    "    id_columns=id_columns,\n",
    "    target_columns=forecast_columns,\n",
    "    control_columns=control_columns,\n",
    "    context_length=context_length,\n",
    "    prediction_length=forecast_horizon,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d126150d-cb95-4d59-a401-2499c0a402e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "# Indices for accessing the OHLC values in the tensors\n",
    "I_OPEN = 0\n",
    "I_CLOSE = 1\n",
    "I_HIGH = 2\n",
    "I_LOW = 3\n",
    "I_VOLUME = 4\n",
    "I_VWAP = 5\n",
    "I_RED = 6\n",
    "I_GREEN = 7\n",
    "\n",
    "\n",
    "def _weighted_average(tensor):\n",
    "    # Get the shape of the tensor\n",
    "    shape = tensor.shape\n",
    "    \n",
    "    # Create a tensor of weights based on the last dimension\n",
    "    weights = torch.arange(shape[-1], 0, -1, dtype=tensor.dtype, device=tensor.device)\n",
    "    \n",
    "    # Multiply the tensor by the weights along the last dimension\n",
    "    weighted_tensor = tensor * weights\n",
    "    \n",
    "    # Sum the weighted tensor along the last dimension\n",
    "    sum_weighted_tensor = torch.sum(weighted_tensor, dim=-1)\n",
    "    \n",
    "    # Sum the weights\n",
    "    sum_weights = torch.sum(weights)\n",
    "    \n",
    "    # Compute the weighted average\n",
    "    weighted_avg = sum_weighted_tensor / sum_weights\n",
    "    \n",
    "    return weighted_avg\n",
    "    \n",
    "\n",
    "def theta_body(y_pred: torch.Tensor, y_obs: torch.Tensor) -> torch.Tensor:\n",
    "    # Create the series of closes\n",
    "    real_candle_closes = y_obs[..., I_CLOSE]\n",
    "    forecasted_candle_closes = y_pred[..., I_CLOSE]\n",
    "\n",
    "    # Get the series of opens\n",
    "    real_candle_opens = y_obs[..., I_OPEN]\n",
    "    forecasted_candle_opens = y_pred[..., I_OPEN]\n",
    "\n",
    "    # Get the series of candle bodies\n",
    "    real_bodies = real_candle_closes - real_candle_opens\n",
    "    forecasted_bodies = forecasted_candle_closes - forecasted_candle_opens\n",
    "\n",
    "    # Get the error of each body\n",
    "    error = real_bodies - forecasted_bodies\n",
    "\n",
    "    sq_error = _weighted_average(torch.square(error))\n",
    "    abs_error = _weighted_average(torch.abs(error))\n",
    "    return sq_error, abs_error\n",
    "\n",
    "def theta_single_pnl(x: torch.Tensor, y_pred: torch.Tensor, y_obs: torch.Tensor) -> torch.Tensor:\n",
    "    # Get the close of the last real candle\n",
    "    last_candle_close = x[..., -1, I_CLOSE]\n",
    "    real_last_candle_close = y_obs[..., -1, I_CLOSE]\n",
    "    real_forecasted_candle_close = y_pred[..., -1, I_CLOSE]\n",
    "\n",
    "    # Compute if the position should be long or short, based on the real values\n",
    "    is_long = real_last_candle_close > last_candle_close\n",
    "\n",
    "    # Compute P/L of long position\n",
    "    pnl_long_real = real_last_candle_close - last_candle_close\n",
    "    pnl_long_forecasted = real_forecasted_candle_close - last_candle_close\n",
    "    # Compute P/L of short position\n",
    "    pnl_short_real = last_candle_close - real_last_candle_close\n",
    "    pnl_short_forecasted = last_candle_close - real_forecasted_candle_close\n",
    "\n",
    "    # Compute the P/L of the real candles vs forecasted candles\n",
    "    pnl_real = torch.where(is_long, pnl_long_real, pnl_short_real)\n",
    "    pnl_forecasted = torch.where(is_long, pnl_long_forecasted, pnl_short_forecasted)\n",
    "\n",
    "    # Compute the P/L residuals. Here if the model's gain is larger than \n",
    "    # the actual gain, the pnl_error will be positive\n",
    "    raw_pnl_error = pnl_forecasted - pnl_real\n",
    "    \n",
    "    # Where the model gives a greater prediction, we want to scale the model's\n",
    "    # P/L error by a log func, to place a greater penalty on losing money over \n",
    "    # predicting too large a gain\n",
    "    error = torch.where(\n",
    "        raw_pnl_error > 0,\n",
    "        torch.log(1 + raw_pnl_error) / 2,\n",
    "        raw_pnl_error\n",
    "    )\n",
    "\n",
    "    sq_error = torch.square(error)\n",
    "    abs_error = torch.abs(error)\n",
    "    return sq_error, abs_error\n",
    "\n",
    "def base_error(y_pred: torch.Tensor, y_obs: torch.Tensor) -> Tuple[torch.Tensor]:\n",
    "    # Place a mask over y_obs, to ensure it is the same size as y_pred\n",
    "    y_obs = y_obs[..., :y_pred.shape[-1]]\n",
    "    \n",
    "    error = y_obs - y_pred\n",
    "\n",
    "    raw_sq_error = torch.mean(torch.square(error), dim=-1)\n",
    "    raw_ae_error = torch.mean((torch.abs(error)), dim=-1)\n",
    "\n",
    "    sq_error = _weighted_average(raw_sq_error)\n",
    "    ae_error = _weighted_average(raw_ae_error)\n",
    "    \n",
    "    return sq_error, ae_error\n",
    "\n",
    "\n",
    "def custom_loss(x: torch.Tensor, y_pred: torch.Tensor, y_obs: torch.Tensor) -> torch.Tensor:\n",
    "    # Compute PNL rediual for each candle\n",
    "    mse, mae = base_error(y_pred, y_obs)\n",
    "    pnl_se, pnl_ae = theta_single_pnl(x, y_pred, y_obs)\n",
    "    body_se, body_ae = theta_body(y_pred, y_obs)\n",
    "\n",
    "    custom_mse = torch.mean(mse + pnl_se + body_se)\n",
    "    custom_mae = torch.mean(mae + pnl_ae + body_ae)\n",
    "\n",
    "    return (custom_mse + custom_mae) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba454c4c-db32-4c4f-8fda-3713c1be1cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices for accessing the OHLC values in the tensors\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "from transformers.models.patchtsmixer.modeling_patchtsmixer import PatchTSMixerForPredictionOutput\n",
    "\n",
    "class MongyModel(PatchTSMixerForPrediction):\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        past_values: torch.Tensor,\n",
    "        observed_mask: Optional[torch.Tensor] = None,\n",
    "        future_values: Optional[torch.Tensor] = None,\n",
    "        output_hidden_states: Optional[bool] = False,\n",
    "        return_loss: bool = True,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> PatchTSMixerForPredictionOutput:\n",
    "        r\"\"\"\n",
    "        observed_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_input_channels)`, *optional*):\n",
    "            Boolean mask to indicate which `past_values` were observed and which were missing. Mask values selected\n",
    "            in `[0, 1]`:\n",
    "                - 1 for values that are **observed**,\n",
    "                - 0 for values that are **missing** (i.e. NaNs that were replaced by zeros).\n",
    "        future_values (`torch.FloatTensor` of shape `(batch_size, target_len, num_input_channels)` for forecasting,:\n",
    "            `(batch_size, num_targets)` for regression, or `(batch_size,)` for classification, *optional*): Target\n",
    "            values of the time series, that serve as labels for the model. The `future_values` is what the\n",
    "            Transformer needs during training to learn to output, given the `past_values`. Note that, this is NOT\n",
    "            required for a pretraining task.\n",
    "\n",
    "            For a forecasting task, the shape is be `(batch_size, target_len, num_input_channels)`. Even if we want\n",
    "            to forecast only specific channels by setting the indices in `prediction_channel_indices` parameter,\n",
    "            pass the target data with all channels, as channel Filtering for both prediction and target will be\n",
    "            manually applied before the loss computation.\n",
    "        return_loss (`bool`,  *optional*):\n",
    "            Whether to return the loss in the `forward` call.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        if self.loss == \"mse\":\n",
    "            # loss = torch.nn.MSELoss(reduction=\"mean\")\n",
    "            loss = custom_loss\n",
    "        elif self.loss == \"nll\":\n",
    "            loss = nll\n",
    "        else:\n",
    "            raise ValueError(\"Invalid loss function: Allowed values: mse and nll\")\n",
    "\n",
    "        return_dict = return_dict if return_dict is not None else self.use_return_dict\n",
    "\n",
    "        # past_values: tensor [batch_size x context_length x num_input_channels]\n",
    "        model_output = self.model(\n",
    "            past_values,\n",
    "            observed_mask=observed_mask,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )  # model_output: [batch_size x nvars x num_patch x d_model]\n",
    "\n",
    "        \n",
    "        if isinstance(model_output, tuple):\n",
    "            model_output = PatchTSMixerModelOutput(*model_output)\n",
    "\n",
    "        # tensor [batch_size x prediction_length x num_input_channels]\n",
    "        y_hat = self.head(model_output.last_hidden_state)\n",
    "\n",
    "        # # Snap the candles to the correct opening positions, before computing the loss\n",
    "        # # This is the \"training wheels\" for the head. By helping the model with the portion\n",
    "        # # of it's task that we can help with, we severly limit the task that is posed to the\n",
    "        # # model\n",
    "        \n",
    "        last_context_close = past_values[..., -1, I_CLOSE]\n",
    "        first_candle_open = y_hat[..., 0, I_OPEN]\n",
    "        first_candle_delta = last_context_close - first_candle_open\n",
    "        first_candle_delta = first_candle_delta.unsqueeze(-1).unsqueeze(-1)\n",
    "        y_hat[..., 0:4] = y_hat[..., 0:4] + first_candle_delta\n",
    "\n",
    "\n",
    "        first_candle_close = y_hat[..., 0, I_CLOSE]\n",
    "        second_candle_open = y_hat[..., 1, I_OPEN]\n",
    "        second_candle_delta = first_candle_close - second_candle_open\n",
    "        second_candle_delta = second_candle_delta.unsqueeze(-1).unsqueeze(-1)\n",
    "        y_hat[..., -2:, 0:4] = y_hat[..., -2:, 0:4] + second_candle_delta\n",
    "\n",
    "        second_candle_close = y_hat[..., 1, I_CLOSE]\n",
    "        third_candle_open = y_hat[..., 2, I_OPEN]\n",
    "        third_candle_delta = second_candle_close - third_candle_open\n",
    "        third_candle_delta = third_candle_delta.unsqueeze(-1)\n",
    "        y_hat[..., -1, 0:4] = y_hat[..., -1, 0:4] + third_candle_delta\n",
    "\n",
    "\n",
    "        loss_val = None\n",
    "        if self.prediction_channel_indices is not None:\n",
    "            if self.distribution_output:\n",
    "                distribution = self.distribution_output.distribution(\n",
    "                    y_hat,\n",
    "                    loc=model_output.loc[..., self.prediction_channel_indices],\n",
    "                    scale=model_output.scale[..., self.prediction_channel_indices],\n",
    "                )\n",
    "                if future_values is not None and return_loss is True:\n",
    "                    loss_val = loss(\n",
    "                        distribution,\n",
    "                        future_values[..., self.prediction_channel_indices],\n",
    "                    )\n",
    "                    # take average of the loss\n",
    "                    loss_val = weighted_average(loss_val)\n",
    "            else:\n",
    "                y_hat = (\n",
    "                    y_hat * model_output.scale[..., self.prediction_channel_indices]\n",
    "                    + model_output.loc[..., self.prediction_channel_indices]\n",
    "                )\n",
    "                if future_values is not None and return_loss is True:\n",
    "                    loss_val = loss(past_values, y_hat, future_values[..., self.prediction_channel_indices])\n",
    "        else:\n",
    "            if self.distribution_output:\n",
    "                distribution = self.distribution_output.distribution(\n",
    "                    y_hat, loc=model_output.loc, scale=model_output.scale\n",
    "                )\n",
    "                if future_values is not None and return_loss is True:\n",
    "                    loss_val = loss(distribution, future_values)\n",
    "                    loss_val = weighted_average(loss_val)\n",
    "            else:\n",
    "                y_hat = y_hat * model_output.scale + model_output.loc\n",
    "                if future_values is not None and return_loss is True:\n",
    "                    loss_val = loss(y_hat, future_values)\n",
    "\n",
    "        if self.prediction_channel_indices is not None:\n",
    "            loc = model_output.loc[..., self.prediction_channel_indices]\n",
    "            scale = model_output.scale[..., self.prediction_channel_indices]\n",
    "        else:\n",
    "            loc = model_output.loc\n",
    "            scale = model_output.scale\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    loss_val,\n",
    "                    y_hat,\n",
    "                    model_output.last_hidden_state,\n",
    "                    model_output.hidden_states,\n",
    "                    loc,\n",
    "                    scale,\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        return PatchTSMixerForPredictionOutput(\n",
    "            loss=loss_val,\n",
    "            prediction_outputs=y_hat,  # tensor [batch_size x prediction_length x num_input_channels]\n",
    "            last_hidden_state=model_output.last_hidden_state,  # x: [batch_size x nvars x num_patch x d_model]\n",
    "            hidden_states=model_output.hidden_states,\n",
    "            loc=loc,\n",
    "            scale=scale,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19456329-1293-45bf-99c7-e5ccf0534846",
   "metadata": {},
   "source": [
    "## Training `PatchTSMixer` From Scratch\n",
    "\n",
    "Adjust the following model parameters according to need.\n",
    "- `d_model` (`int`, *optional*, defaults to 8):\n",
    "    Hidden dimension of the model. Recommended to set it as a multiple of patch_length (i.e. 2-8X of\n",
    "    patch_len). Larger value indicates more complex model.\n",
    "- `expansion_factor` (`int`, *optional*, defaults to 2):\n",
    "    Expansion factor to use inside MLP. Recommended range is 2-5. Larger value indicates more complex model.\n",
    "- `num_layers` (`int`, *optional*, defaults to 3):\n",
    "    Number of layers to use. Recommended range is 3-15. Larger value indicates more complex model.\n",
    "- `mode`: (`str`, either to 'common_channel' or `mix_channel`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "226b904e-1ab2-478b-98b4-ce99bc23f1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_length = 16\n",
    "stride_length = 1\n",
    "\n",
    "prediction_channel_indicies = train_tsp.prediction_channel_indices\n",
    "num_input_channels = train_tsp.num_input_channels\n",
    "\n",
    "config = PatchTSMixerConfig(\n",
    "    # Dataset Kwargs\n",
    "    context_length=context_length,\n",
    "    prediction_length=forecast_horizon,\n",
    "    prediction_channel_indices=prediction_channel_indicies,\n",
    "    patch_length=patch_length,\n",
    "    num_input_channels=num_input_channels,\n",
    "    patch_stride=stride_length,\n",
    "\n",
    "    # Model Kwargs\n",
    "    d_model=6 * patch_length,\n",
    "    num_layers=4,\n",
    "    expansion_factor=3,\n",
    "    dropout=0.5,\n",
    "    head_dropout=0.7,\n",
    "    mode=\"mix_channel\",\n",
    "    scaling=None,\n",
    ")\n",
    "model = MongyModel(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fae7e0c",
   "metadata": {},
   "source": [
    "# Training Run Summaries\n",
    "\n",
    "**Run 1**: (N/A)\n",
    "This run used the full year of data, and was used as a baseline to establish that the `mix_channel` mode is more effective for our task. Additionally, all subsequent runs have been updated, to instead use only the first three months of data as training data. Thus, while the loss for this run is lower, it is not indicaitve of the paramters being a better fit, just a result of having a larger dataset.\n",
    "\n",
    "**Run 2** (0.108476):\n",
    "This run was the first in which only the first two months of data was used as a training set. March was then split in half to form the validation and test sets. Additionally, the context window was expanded, to include the last four hours of data. While this wasn't explicitly compared against a shorter context window with the same dataset, the results of the paper provide an incredibly strong suggestions towards this approach yielding more effective performance.\n",
    "\n",
    "**Run 3** (0.108230):\n",
    "This run included involved increasing the `num_layers` argument from 3 to 5. This adds additional layers to the model, giving it more of an ability to percieve complex patterns in the financial data. This results in a larger model, but hopefully, will allow the model to better understand the nuances of the highly complex financial data it is being trained on.\n",
    "\n",
    "**Run 4**: (0.107247)\n",
    "This run included further incrementing the `num_layers` argument from 5 to 10. This adds additional further layers to capture more of the complex patterns in the financial dataset. \n",
    "\n",
    "_NOTE_: The `num_layers` does not seem to provide additional aid in this trainin task, with the side-effect of signifitcanlty increasing the inference time. As a result, we are making the decision to keep `num_layers = 3`.\n",
    "\n",
    "---\n",
    "\n",
    "**Run 5**: (0.108397)\n",
    "The `num_layers` argument has been reset to a value of 3, which returns our baseline back to _Run 2_. The `expansion_factor` has been increased from 3 to 4. This yeilded a slight decrease in validation loss, so potentially worth running a second experiment, but likely best to test patching instead.\n",
    "\n",
    "**Run 6** ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27812e8c-c0f6-45e3-a075-310929329460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing forecasting training on Dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45000' max='207000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 45000/207000 5:36:05 < 20:09:59, 2.23 it/s, Epoch 21/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.278800</td>\n",
       "      <td>1.088569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.929100</td>\n",
       "      <td>1.057608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.673900</td>\n",
       "      <td>1.039402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.463400</td>\n",
       "      <td>1.026534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>2.279800</td>\n",
       "      <td>1.016420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>2.122400</td>\n",
       "      <td>1.008408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>1.978800</td>\n",
       "      <td>1.001682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>1.864100</td>\n",
       "      <td>0.996259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>1.758400</td>\n",
       "      <td>0.991706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>1.671000</td>\n",
       "      <td>0.988094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>1.593600</td>\n",
       "      <td>0.985499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>1.531000</td>\n",
       "      <td>0.983700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>1.473300</td>\n",
       "      <td>0.982698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>1.422600</td>\n",
       "      <td>0.982128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>1.382800</td>\n",
       "      <td>0.981290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>1.346600</td>\n",
       "      <td>0.980882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>1.316800</td>\n",
       "      <td>0.980876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>1.286200</td>\n",
       "      <td>0.980529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=45000, training_loss=1.9095892957899305, metrics={'train_runtime': 20168.5805, 'train_samples_per_second': 656.625, 'train_steps_per_second': 10.263, 'total_flos': 7.27042812961536e+16, 'train_loss': 1.9095892957899305, 'epoch': 21.73913043478261})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the run number\n",
    "run_num = \"morning_4\"\n",
    "save_dir = f\"./checkpoints/run_{run_num}\"\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Check if save_dir exists\n",
    "assert not os.path.exists(save_dir), \"Please update the run_num to avoid overwriting checkpoints!\"\n",
    "\n",
    "num_workers = 10  # p3.2xlarge instance has 12 vCPUs\n",
    "\n",
    "gradient_accumulation_steps = 1 # Number of batches between each backward pass\n",
    "batch_size = 64 # Size of each batches sent to GPU\n",
    "eval_batch_size = 256\n",
    "num_steps = 2500\n",
    "\n",
    "# Calculations\n",
    "# =======================\n",
    "# effective_batch_size = batch_size * grad_accumulation_steps = 64 * 1 = 64\n",
    "# examples_per_evaluation = num_steps * effective_batch_size = 64 * 2,500 = 160,0000\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=f\"{save_dir}/output/\",\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=0.000001,\n",
    "    weight_decay=0.0000005,\n",
    "    num_train_epochs=100,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=num_steps,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    eval_accumulation_steps=250,\n",
    "    dataloader_num_workers=num_workers,\n",
    "    report_to=\"tensorboard\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=num_steps,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=num_steps,\n",
    "    save_total_limit=3,\n",
    "    logging_dir=f\"{save_dir}/logs/\",  # Make sure to specify a logging directory\n",
    "    load_best_model_at_end=True,  # Load the best model when training ends\n",
    "    metric_for_best_model=\"eval_loss\",  # Metric to monitor for early stopping\n",
    "    greater_is_better=False,  # For loss\n",
    "    label_names=[\"future_values\"], # The names of the \"ground truth\" values to compare predictions against\n",
    ")\n",
    "\n",
    "# Create a new early stopping callback with faster convergence properties\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=5,  # Number of epochs with no improvement after which to stop\n",
    "    early_stopping_threshold=0.001,  # Minimum improvement required to consider as improvement\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    callbacks=[early_stopping_callback],\n",
    ")\n",
    "\n",
    "print(\"Doing forecasting training on Dataset\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e06f3931-6b5f-450e-b17d-360ae3984e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='66' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [66/66 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.0075856447219849,\n",
       " 'eval_runtime': 29.835,\n",
       " 'eval_samples_per_second': 557.834,\n",
       " 'eval_steps_per_second': 2.212,\n",
       " 'epoch': 21.73913043478261}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fd01ef-5b6c-4bac-bc42-25052c17b45f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-kernel",
   "language": "python",
   "name": "venv-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
